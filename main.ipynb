{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Learn\n",
    "Deep learning with numpy\n",
    "\n",
    "this notebook is based on &lt;Deep Learning from scratch&gt; by Saito Goki.\\\n",
    "but all codes are heavly modified for easy understanding. Thanks to O'reilly\n",
    "\n",
    "(c) Minhyeok Park. 2024. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==2.4.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: cycler==0.12.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: debugpy==1.8.7 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.8.7)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: executing==2.1.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: fonttools==4.54.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.54.1)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.28.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (8.28.0)\n",
      "Requirement already satisfied: jedi==0.19.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.19.1)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (5.7.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (1.4.7)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.9.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.1.2 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (2.1.2)\n",
      "Requirement already satisfied: packaging==24.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (24.1)\n",
      "Requirement already satisfied: parso==0.8.4 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.0.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (11.0.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (3.0.48)\n",
      "Requirement already satisfied: psutil==6.1.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (6.1.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.2.3)\n",
      "Requirement already satisfied: Pygments==2.18.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (2.18.0)\n",
      "Requirement already satisfied: pyparsing==3.2.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (26.2.0)\n",
      "Requirement already satisfied: six==1.16.0 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 32)) (1.16.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 33)) (0.6.3)\n",
      "Requirement already satisfied: tornado==6.4.1 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (6.4.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 35)) (5.14.3)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./env/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (0.2.13)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론\n",
    "딥러닝, 신경망에서 우리가 생각할 수 있는 가장 최소 단위의 함수를 퍼셉트론(단일 퍼셉트론)이라 한다\n",
    "\n",
    "퍼셉트론은 다음과 같은 간단한 함수이며 여러 데이터 값을 받아 하나의 출력을 하는 함수로 인간의 뇌에서 신경 세포와 비슷한 구조를 가진다.\n",
    "\n",
    "```\n",
    "y={0 (w1*x1+w2*x2 <= θ)}\n",
    "  {1 (w1*x1+w2*x2 > θ)}\n",
    "```\n",
    "\n",
    "받은 백터 x에 각각 일치하는 가중치 w를 곱한 후 해당 값이 특정 임계값 θ를 넘어가면 0, 아니면 1을 출력한다 \\\n",
    "파이썬으로 작성하면 다음과 같다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_1 (x1, x2):\n",
    "  w1, w2, theta = a, b, c # type: ignore\n",
    "\n",
    "  if x1*w1 + x2*w2 <= theta:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 이때 가중치 w와 임계값 θ를 잘 활용하면 여러 함수들을 만들 수 있게 된다. \\\n",
    "다음은 AND 연산을 해주는 퍼셉트론이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 = 0\n",
      "1 AND 0 = 0\n",
      "0 AND 1 = 0\n",
      "1 AND 1 = 1\n"
     ]
    }
   ],
   "source": [
    "def and_perceptron (x1, x2):\n",
    "  w1, w2, theta = 0.5, 0.5, 0.7\n",
    "\n",
    "  if x1*w1 + x2*w2 <= theta:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "  \n",
    "print('0 AND 0 =', and_perceptron(0, 0))\n",
    "print('1 AND 0 =', and_perceptron(1, 0))\n",
    "print('0 AND 1 =', and_perceptron(0, 1))\n",
    "print('1 AND 1 =', and_perceptron(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 퍼셉트론을 쫌 더 재밌게 만들어 보자. \\\n",
    "앞서 봤던 퍼셉트론의 `(w1*x1+w2*x2 <= θ)` 부분을 다음과 같이 표현할 수 있다.\n",
    "\n",
    "\n",
    "```\n",
    "y={0 (w1*x1+w2*x2+b <= 0)}\n",
    "  {1 (w1*x1+w2*x2+b > 0)}\n",
    "```\n",
    "\n",
    "부등호 뒤에 있던 임계값 θ를 앞으로 옮겼을 뿐이다. 이제 우리는 그것을 편향 b라고 부른다.\\\n",
    "당연히 b 값은 이제 부호를 반대로 하여 주어야 한다.\n",
    "\n",
    "개선된 식으로 가독성 높은 코드와 빠른 속도를 위해 numpy 라이브러리를 통해 AND, NAND, OR 게이트를 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 = 0\n",
      "1 AND 0 = 0\n",
      "0 AND 1 = 0\n",
      "1 AND 1 = 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def and_perceptron2 (x: np.ndarray):\n",
    "  w = np.array([0.5, 0.5])\n",
    "  b = -0.7\n",
    "\n",
    "  if np.sum(w*x) + b <= 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "  \n",
    "print('0 AND 0 =', and_perceptron2(np.array([0, 0])))\n",
    "print('1 AND 0 =', and_perceptron2(np.array([1, 0])))\n",
    "print('0 AND 1 =', and_perceptron2(np.array([0, 1])))\n",
    "print('1 AND 1 =', and_perceptron2(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NAND 0 = 1\n",
      "1 NAND 0 = 1\n",
      "0 NAND 1 = 1\n",
      "1 NAND 1 = 0\n"
     ]
    }
   ],
   "source": [
    "def nand_perceptron (x: np.ndarray):\n",
    "  w = np.array([-0.5, -0.5])\n",
    "  b = 0.7\n",
    "\n",
    "  if np.sum(w*x) + b <= 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "  \n",
    "print('0 NAND 0 =', nand_perceptron(np.array([0, 0])))\n",
    "print('1 NAND 0 =', nand_perceptron(np.array([1, 0])))\n",
    "print('0 NAND 1 =', nand_perceptron(np.array([0, 1])))\n",
    "print('1 NAND 1 =', nand_perceptron(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0 = 0\n",
      "1 OR 0 = 1\n",
      "0 OR 1 = 1\n",
      "1 OR 1 = 1\n"
     ]
    }
   ],
   "source": [
    "def or_perceptron (x: np.ndarray):\n",
    "  w = np.array([0.5, 0.5])\n",
    "  b = -0.2\n",
    "\n",
    "  if np.sum(w*x) + b <= 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "  \n",
    "print('0 OR 0 =', or_perceptron(np.array([0, 0])))\n",
    "print('1 OR 0 =', or_perceptron(np.array([1, 0])))\n",
    "print('0 OR 1 =', or_perceptron(np.array([0, 1])))\n",
    "print('1 OR 1 =', or_perceptron(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 AND, NAND, OR 게이트를 퍼셉트론을 통해 구현할 수 있다.\n",
    "\n",
    "이처럼 함수의 수정 없이 가중치 w와 편향 b만을 수정하기만 하면 여러 상황들을 만들어 낼 수 있는 것이 퍼셉트론의 장점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론\n",
    "퍼셉트론 하나만으론 XOR 게이트를 구현할 수 없다.\n",
    "\n",
    "퍼셉트론은 x{1}과 x{2}의 축을 가진 그래프에서 가상의 직선을 하나 그어 0과 1 값을 구분하는 함수이다. \\\n",
    "하지만 XOR은 직선 하나만으로 구분할 수 없고 (즉 선형으로는 해결할 수 없고) \\\n",
    "직선이 아닌 선으로 (비선형으로) 그어야 구분할 수 있다.\n",
    "\n",
    "이 비선형 구분을 하는 XOR 게이트와 같은 알고리즘은 여러 퍼셉트론을 결합하여 구현할 수 있다. \\\n",
    "마치 AND, NAND, OR 게이트로 XOR 게이트를 만들 수 있는 것 처럼 말이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 = 0\n",
      "1 XOR 0 = 1\n",
      "0 XOR 1 = 1\n",
      "1 XOR 1 = 0\n"
     ]
    }
   ],
   "source": [
    "def xor_perceptron(x: np.ndarray):\n",
    "  y = np.array([nand_perceptron(x), or_perceptron(x)])\n",
    "  return and_perceptron2(y)\n",
    "\n",
    "\n",
    "print('0 XOR 0 =', xor_perceptron(np.array([0, 0])))\n",
    "print('1 XOR 0 =', xor_perceptron(np.array([1, 0])))\n",
    "print('0 XOR 1 =', xor_perceptron(np.array([0, 1])))\n",
    "print('1 XOR 1 =', xor_perceptron(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR을 구현하는 이 다층 퍼셉트론은 2개의 층으로 구성되어 있다.\n",
    "\n",
    "1층: NAND 퍼셉트론과 OR 퍼셉트론이 있는 층 \\\n",
    "2층: AND 퍼셉트론이 있는 층\n",
    "\n",
    "이렇게 2층을 쌓는 것 만으로도 기존 단일 퍼셉트론으론 할 수 없었던 연산을 가능하게 되었다!\\\n",
    "이 뜻은 퍼셉트론을 여러겹 쌓게 되면 선형적 표현을 넘어서 비선형적 표현도 가능하다는 의미이다.\n",
    "\n",
    "실제로 이것을 응용하면 인간이 생각할 수 있는 모든 회로를 단 2층의 퍼셉트론으로 모두 구현할 수 있다. (w와 b를 찾는게 매우 어렵겠지만)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망\n",
    "위 퍼셉트론에서 문제점이 무엇일까? 바로 사람이 직접 w와 b를 찾아줘야 했던 점이다. \\\n",
    "신경망은 퍼셉트론에 더 많은 기능을 추가하여 자동으로 w와 b를 찾아주는 기능을 구현한다.\n",
    "\n",
    "아까 구현했던 XOR 회로를 기준으로 신경망의 층을 나눠보자.\n",
    "\n",
    "입력층: 입력받았던 x가 있는 층 \\\n",
    "은닉층 1층: NAND와 OR 퍼셉트론이 있던 층 \\\n",
    "출력층: AND 퍼셉트론이 있던 층\n",
    "\n",
    "신경망에서 입력을 받는 층을 입력층, 내부적인 처리를 하는 은닉층, 최종적으로 결과를 계산하는 출력층이 존재한다. \\\n",
    "이때 입력층을 포함해서 3개니까 3층 신경망이 아니냐고 예기를 할 수 있겠지만 w와 b를 계산하지 않는 입력층은 포함되지 않아 2층 신경망이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수\n",
    "이제 전의 퍼셉트론 함수를 쫌 더 재미있게 만들어 보자 \\\n",
    "`y={0 (w1*x1+w2*x2+b <= 0)}` 에서 결과 값이 0보다 큰지 작은지 체크하는 부분을 따로 때어 보는것이다.\n",
    "\n",
    "```\n",
    "y=h(b+w1*x1+w2+x2)\n",
    "h={0 (x <= 0)}\n",
    "  {1 (x > 0) }\n",
    "```\n",
    "\n",
    "`b+w1*x1+w2+x2` 의 내용이 0보다 큰지 작은지를 계산하는 부분을 함수 h로 때어내었다.\\\n",
    "이때 이 함수 h는 활성화 함수라고 부른다.\n",
    "\n",
    "구현한 코드를 보면 더 쉽게 이해할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 = 0\n",
      "1 AND 0 = 0\n",
      "0 AND 1 = 0\n",
      "1 AND 1 = 1\n"
     ]
    }
   ],
   "source": [
    "def h1(x):\n",
    "  if x <= 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "\n",
    "# numpy를 사용해 배열도 지원하고 짧게 구현해보자\n",
    "def h2(x: np.ndarray):\n",
    "  y = x > 0 # 입력받은 배열 x의 모든 값을 0과 비교하고 True, False로 반환한다\n",
    "\n",
    "  return y.astype(int) # boolean을 0과 1로 변환한다.\n",
    "\n",
    "# 더 짧게 구현할 수 있다.\n",
    "def h3(x: np.ndarray):\n",
    "  return np.array(x > 0, dtype=int)\n",
    "\n",
    "def and_perceptron3 (x: np.ndarray):\n",
    "  w = np.array([0.5, 0.5])\n",
    "  b = -0.7\n",
    "  y = h3(np.array([np.sum(w*x) + b]))\n",
    "\n",
    "  return y\n",
    "  \n",
    "print('0 AND 0 =', and_perceptron2(np.array([0, 0])))\n",
    "print('1 AND 0 =', and_perceptron2(np.array([1, 0])))\n",
    "print('0 AND 1 =', and_perceptron2(np.array([0, 1])))\n",
    "print('1 AND 1 =', and_perceptron2(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 활성화 함수의 분리 여부는 퍼셉트론이 신경망으로 진화하는데 큰 역할을 한다.\n",
    "\n",
    "왜냐면 0과 1을 리턴했던 재미없는 함수(계단 함수) h를 재미있게 수정할 수 있기 때문이다! \\\n",
    "일단 먼저 이때까지 사용했던 계단 함수를 그래프를 통해 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = h3(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 재미없는 선형 함수를 활성화 함수로 하게 되면 신경망의 이점을 얻을 수 없다.\n",
    "\n",
    "마치 `h=cx`로 함수를 만들고 `y=h(h(h(x)))` 라고 한다면 굳이 저렇게 적지 말고 `y=ax, a=c^3` 이라고만 하면 되는 것 처럼 \\\n",
    "중첩의 의미를 가질 수 없게 되어 쓰는 의미가 없어지기 때문이다.\n",
    "\n",
    "그럼 비선형적 함수를 한번 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시그모이드 함수\n",
    "이번엔 쫌 재밌는 활성화 함수를 만들어 보자. 직선이 가득한 선형적 함수가 아닌\\\n",
    "비선형적 함수를 만들어보는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x: np.ndarray):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ReLU 함수\n",
    "시그모이드 함수를 많이 사용해 왔지만 최근 더 많이 사용되고 있는 ReLU 함수는 다음과 같다\\\n",
    "ReLU 함수는 수식으로 표현하면 다음과 같으며 간단히 max를 통해 구현할 수 있다\n",
    "\n",
    "```\n",
    "h={x (x >= 0)}\n",
    " ={0 (x <  0)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(x: np.ndarray):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-1, 5.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬의 점곱\n",
    "신경망 연산을 이해하기 위해선 행렬의 점곱에 대해서 먼저 알아야 한다.\\\n",
    "A행렬과 B행렬에 대한 곱셈을 할때에는 그냥 원자끼리 곱한다고 해결되는 것이 아니라\\\n",
    "구하고자 하는 x 원소가 위치하는 A 행렬의 모든 행과 x 원소가 위치하는 B 행렬의 모든 열을 곱한뒤 더해 주어야 한다.\n",
    "\n",
    "즉 다음과 같은 식이 있을때 x를 구하려면 \n",
    "\n",
    "```\n",
    "{ 1 2 }   { 5 6 }   { x ? }\n",
    "{ 3 4 } * { 7 8 } = { ? ? }\n",
    "\n",
    "x=1*5+2*7=19\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([\n",
    "  [1, 2],\n",
    "  [3, 4]\n",
    "])\n",
    "\n",
    "B=np.array([\n",
    "  [5, 6],\n",
    "  [7, 8]\n",
    "])\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 행렬의 점곱을 해보면서 알 수 있는 사실이 하나 있다.\\\n",
    "바로 A 행렬과 B 행렬을 점곱하려면 A의 행 수와 B의 열 수가 일치해야 한다는 점이다.\n",
    "\n",
    "이는 다음과 같이 확인해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([\n",
    "  [1, 2],\n",
    "  [3, 4]\n",
    "])\n",
    "\n",
    "B=np.array([\n",
    "  [5, 6],\n",
    "  [7, 8],\n",
    "  [7, 8]\n",
    "])\n",
    "\n",
    "\n",
    "# 각 행렬의 크기:\n",
    "# A = 2x2\n",
    "# B = 3x2\n",
    "\n",
    "# np.dot(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19],\n",
       "       [43]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([\n",
    "  [1, 2],\n",
    "  [3, 4]\n",
    "])\n",
    "\n",
    "B=np.array([\n",
    "  [5],\n",
    "  [7]\n",
    "])\n",
    "\n",
    "\n",
    "# 각 행렬의 크기:\n",
    "# A = 2x2\n",
    "# B = 2x1\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 A의 행과 B의 열 수를 맞추지 않은 두 행렬의 대한 점곱은 오류가 발생하는 것을 볼 수 있으며\\\n",
    "일치할 경우 오류가 발생하지 않는 것을 볼 수 있다. 이는 두 행렬의 차원 수가 달라도 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 43])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([\n",
    "  [1, 2],\n",
    "  [3, 4]\n",
    "])\n",
    "\n",
    "B=np.array([5, 7])\n",
    "\n",
    "\n",
    "# 각 행렬의 크기:\n",
    "# A = 2x2\n",
    "# B = 2\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬의 접곱을 통한 퍼셉트론\n",
    "위의 `x=1*5+2*7` 식에서 퍼셉트론의 모양이 보이지 않는가?\\\n",
    "그렇다 퍼셉트론 식 `y=h(w1*x1+w2*x2+b)`는 행렬곱을 통해서 더 간단히 표현할 수 있다.\n",
    "\n",
    "```\n",
    "X=(입력 x값의 행렬)\n",
    "W=(가중치 w값의 행렬)\n",
    "B=(편향 b가 하나 있는 행렬)\n",
    "A=(결과값 y값의 행렬)\n",
    "\n",
    "A=h(XW+B)\n",
    "```\n",
    "\n",
    "이 규칙을 통해 임의의 x와 w, b로 신경망 1층을 만들어보자.\n",
    "\n",
    "입력 2개의 대한 퍼셉트론 3개를 가진 층이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57444252, 0.66818777, 0.75026011])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([1.0, 0.5])        # 입력 층 입력값 (크기: 2)\n",
    "W1=np.array([                 # 1층(퍼셉트론 3개)의 대한 입력값(2개) 각각의 가중치 (크기: 2x3)\n",
    "  [0.1, 0.3, 0.5],\n",
    "  [0.2, 0.4, 0.6]\n",
    "])\n",
    "B1=np.array([0.1, 0.2, 0.3])  # 1층(퍼셉트론 3개)의 대한 각각의 편향 (크기: 3)\n",
    "\n",
    "h=sigmoid                     # 활성화 함수로 시그모이드 함수 사용\n",
    "A1=h(np.dot(X, W1) + B1)      # 1층의 퍼셉트론 3개의 결과값 (크기: 3)\n",
    "\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1층의 결과값 3개를 다음 층에 넘겨보자\n",
    "\n",
    "1층 결과값 3개의 대한 2개의 퍼셉트론을 가진 층이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62624937, 0.7710107 ])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2=np.array([                 # 2층(퍼셉트론 2개)의 대한 1층값(3개) 각각의 가중치 (크기: 3x2)\n",
    "  [0.1, 0.4],\n",
    "  [0.2, 0.5],\n",
    "  [0.3, 0.6]\n",
    "])\n",
    "B2=np.array([0.1, 0.2])       # 2층(퍼셉트론 2개)의 대한 각각의 편향 (크기: 2)\n",
    "\n",
    "h=sigmoid                     # 활성화 함수로 시그모이드 함수 사용\n",
    "A2=h(np.dot(A1, W2) + B2)     # 2층의 퍼셉트론 2개의 결과값 (크기: 2), 1층의 출력값을 사용\n",
    "\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 2층의 결과값 2개를 출력 층 (퍼셉트론 2개)에 넘겨보자\\\n",
    "이때 출력층은 활성화 함수로 입력 받은것을 그대로 출력하는 항등 함수를 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31682708, 0.69627909])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identity(x: np.ndarray):\n",
    "  return x\n",
    "\n",
    "W3=np.array([                 # 출력층(퍼셉트론 2개)의 대한 2층값(2개) 각각의 가중치 (크기: 2x2)\n",
    "  [0.1, 0.3],\n",
    "  [0.2, 0.4]\n",
    "])\n",
    "B3=np.array([0.1, 0.2])       # 출력층(퍼셉트론 2개)의 대한 각각의 편향 (크기: 2)\n",
    "\n",
    "h=identity                    # 활성화 함수로 항등 함수 사용\n",
    "A3=h(np.dot(A2, W3) + B3)     # 출력층의 퍼셉트론 2개의 결과값 (크기: 2), 2층의 출력값을 사용\n",
    "\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 3층 신경망을 구현해 보았다. \\\n",
    "방금처럼 출력층에 경우 원하는 출력 형태의 따라 은닉층과 다른 함수를 사용하는 경우가 많다.\n",
    "\n",
    "Regression (회귀) = 항등 함수\\\n",
    "Classification (분류) = 시그모이드 (클래스 2개), 소프트맥스 (클래스 2개 이상)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구현 정리\n",
    "앞에서 했던 신경망 구현을 한 스크립트로 모으면 다음과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31682708, 0.69627909])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1=np.array([0.1, 0.2, 0.3])\n",
    "W1=np.array([\n",
    "  [0.1, 0.3, 0.5],\n",
    "  [0.2, 0.4, 0.6]\n",
    "])\n",
    "\n",
    "b2=np.array([0.1, 0.2])\n",
    "W2=np.array([\n",
    "  [0.1, 0.4],\n",
    "  [0.2, 0.5],\n",
    "  [0.3, 0.6]\n",
    "])\n",
    "\n",
    "b3=np.array([0.1, 0.2])\n",
    "W3=np.array([\n",
    "  [0.1, 0.3],\n",
    "  [0.2, 0.4]\n",
    "])\n",
    "\n",
    "# ---\n",
    "\n",
    "def forward(x: np.ndarray):\n",
    "  a1=sigmoid(np.dot(x, W1) + b1)\n",
    "  a2=sigmoid(np.dot(a1, W2) + b2)\n",
    "  a3=identity(np.dot(a2, W3) + b3)\n",
    "\n",
    "  return a3\n",
    "\n",
    "# ---\n",
    "\n",
    "x=np.array([1.0, 0.5])\n",
    "y=forward(x)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 함수\n",
    "분류 문제에서 사용되는 소프트맥스 활성화 함수는 한가지 다른점이 있다.\\\n",
    "다른 활성화 함수의 경우 xW+b의 대한 결과값만 입력으로 취하지만 소프트맥스 함수의 경우\\\n",
    "해당 함수가 속하는 층의 모든 퍼셉트론의 결과값을 참고하여 활성화 결과를 출력한다.\n",
    "\n",
    "소프트맥스 함수의 경우 e 값을 적극 사용하며 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40625907, 0.59374093])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 소프트맥스 함수의 특징은 입력값의 대소 관계가 그대로 유지되면서도\\\n",
    "출력값이 0과 1 사이의 값들로 다 합칠 경우 1로 떨어지는 특징이 있다.\n",
    "\n",
    "이는 분류 문제에서 유용하게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 손글씨 데이터셋의 대한 신경망 구현\n",
    "미리 계산된(학습된) W값과 b값들을 통해 신경망을 구현해보자\n",
    "\n",
    "이렇게 W값과 b값이 정해진 상태에서 입력을 신경망에 태우는 것을 Forward propagation, 순전파 라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, t) = mnist_loader.load_dataset(normalize=True, flatten=True, one_hot_label=False)\n",
    "network = mnist_loader.load_pretrained()\n",
    "\n",
    "def predict(network, x):\n",
    "  W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "  b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "  z1 = sigmoid(np.dot(x, W1) + b1)\n",
    "  z2 = sigmoid(np.dot(z1, W2) + b2)\n",
    "  y = softmax(np.dot(z2, W3) + b3)\n",
    "\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 이 생성한 신경망에 테스트 데이터를 순전파해\\\n",
    "몇 %나 정답을 맞췄는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.9352'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "  y = predict(network, x[i]) # 예측 시작\n",
    "  p = np.argmax(y)           # softmax 값이 가장 높은 원소 추출\n",
    "\n",
    "  if p == t[i]:              # 예측 성공하면 1 증가\n",
    "    correct_count += 1\n",
    "\n",
    "f\"Accuracy: {correct_count / len(x)}\" # 모든 데이터 중 맞는것의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93.52% 의 정확도로 예측하는데 성공했다. 93%면 높은것 같지만 나중에는 더 정확도를 높혀볼 것이다.\n",
    "\n",
    "그나저나 위의 스크립트는 데이터 셋을 하나하나 forward 했다.\\\n",
    "더 빠른 속도를 위해(numpy의 최적화 설계에서 최대한 이점을 얻기 위해) 한꺼번에 100개씩 forward해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.9352'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "correct_count = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):    # i는 이제 100씩 증가한다.\n",
    "  y = predict(network, x[i:i+batch_size]) # i부터 i+100까지 한꺼번에 예측 시작\n",
    "  p = np.argmax(y, axis=1)                # softmax 값이 가장 높은 원소 100개 추출\n",
    "\n",
    "  correct_count += np.sum(p == t[i:i+batch_size]) # np.sum에서 True가 1으로 변환되는 것을 활용해서 맞은 개수를 한꺼번에 계산\n",
    "\n",
    "f\"Accuracy: {correct_count / len(x)}\" # 모든 데이터 중 맞는것의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 인코딩\n",
    "MNIST 데이터셋을 불러올때 One-hot 인코딩을 할것인가 말것인가 선택하는 부분이 있었다.\\\n",
    "위 코드에서는 사용하지 않았지만 나중에 one-hot 인코딩을 사용할때를 대비해 무엇인지 알아보자\n",
    "\n",
    "다음과 같은 테이블이 있다고 생각해보자\n",
    "```\n",
    "| name | age | y                  |\n",
    "|------|-----|--------------------|\n",
    "| pmh  | 19  | Computer Science   |\n",
    "| ljw  | 18  | Art                |\n",
    "| cth  | 19  | Blockchain         |\n",
    "```\n",
    "\n",
    "y를 One-hot 인코딩하였을때 다음과 같아진다.\n",
    "```\n",
    "| name | age | y (Computer Science) | y (Art) | y (Blockchain) |\n",
    "|------|-----|----------------------|---------|----------------|\n",
    "| pmh  | 19  | True                 | False   | False          |\n",
    "| ljw  | 18  | False                | True    | False          |\n",
    "| cth  | 19  | False                | False   | True           |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 학습\n",
    "위에서 제작한 신경망이 매우 많은 층을 이루고 있거나 많은 퍼셉트론을 가지고 있다면 사람이 손수 w값과 값을 알아맞추는 것은 매우 어려울 것이다.\\\n",
    "각 퍼셉트론의 w값과 b값 같은 파라미터들을 신경망이 스스로 조정하는 것을 우리는 신경망 학습이라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 신경망이란?\n",
    "머신러닝은 입력값의 특징 추출 알고리즘을 사람이 생각해서 결정한다는 특징이 있다.\\\n",
    "예를 들어 SIFT나 HOG같은 알고리즘을 원본 데이터에 적용시키고 SVM, KNN 같은 뉴럴 네트워크를 진행하는데 있다.\n",
    "\n",
    "하지만 딥러닝은 머신러닝과 다르게 종단간 머신러닝, 즉 처음부터 끝까지 신경망을 통해 진행한다.\\\n",
    "입력을 그대로 받아 신경망에 적용시킨다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 시험 데이터\n",
    "딥러닝을 훈련시키기 위해서는 두가지의 데이터셋이 필요하다\n",
    "\n",
    "훈련 데이터는 딥러닝 신경망을 학습시키기 위한 데이터로 신경망은 이것을 토대로 파라미터들을 조정하게 된다.\\\n",
    "시험 데이터는 훈련 데이터와 구분된 데이터셋으로 신경망이 입력받은 훈련 데이터에게만 적합한 과적합 상태가 되지 않도록 테스트하기 위해 존재한다.\n",
    "\n",
    "또한 이렇게 신경망이 훈련 데이터에만 치중되어 범용적이지 못하게 되는 것을 오버피팅이라고 예기한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수\n",
    "손실함수는 현재의 파라미터로 훈련 데이터에서 얼마나 예측에 실패했냐를 의미하는 함수이다.\n",
    "\n",
    "신경망은 손실 함수의 값을 통하여 현재 자신이 얼마나 더 수정되어야 하는지 결정하게 된다.\\\n",
    "사실 어려운게 아니라 y 배열과 t 배열을 받았을때 두 배열이 얼마나 차이나냐~ 라는 말이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차\n",
    "평균 제곱 오차(MSE) 함수는 손실 함수 중에서 가장 많이 사용되는 함수로\n",
    "\n",
    "파이썬 코드로 다음과 같이 작성할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "  return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 y는 모델이 예측한 결과 값이며 t는 실제 정답 값이다.\n",
    "\n",
    "Class가 5개인 Classification 문제에서 다음과 같은 y와 t가 나왔을때 MSE값을 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4262500000000001)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0]) # 정답은 class 2\n",
    "y = np.array([0.4, 0.1, 0.2, 0.05, 0.2]) # 정답이 class 0이라고 예측함\n",
    "\n",
    "mean_squared_error(y, t) # 0.4 정도로 손실률이 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.12625000000000003)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0.2, 0.1, 0.6, 0.05, 0.2]) # 정답이 class 2이라고 예측함\n",
    "\n",
    "mean_squared_error(y, t) # 0.1 정도로 손실률이 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 정답과 가까워질수록 손실함수의 결과값이 낮아지는 것을 볼 수 있다.\n",
    "\n",
    "신경망은 이 값을 기준으로 학습을 진행하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교차 엔트로피 오차\n",
    "또 다른 함수인 교차 엔트로피 오차를 살펴보자. 이것도 자주 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6094374124342252)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "  delta = 1e-7 # log(0)으로 인해 -Infinity가 나지 않도록 매우 작은 값 추가\n",
    "  return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "y = np.array([0.4, 0.1, 0.2, 0.05, 0.2]) # 정답이 class 0이라고 예측함\n",
    "\n",
    "cross_entropy_error(y, t) # 1.6 정도로 손실률이 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.510825457099338)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0.2, 0.1, 0.6, 0.05, 0.2]) # 정답이 class 2이라고 예측함\n",
    "\n",
    "cross_entropy_error(y, t) # 0.5 정도로 손실률이 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배치값을 위한 교차 엔트로피 오차\n",
    "이때까지의 손실 함수는 하나의 데이터의 대한 손실 함수이다.\\\n",
    "만약 배치 작업을 해서 여러 데이터의 대한 결과가 나왔을때 그 모든것에 대한 손실 함수는 다음과 같이 짤 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2432334273225962)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y: np.ndarray, t: np.ndarray):\n",
    "  if y.ndim == 1:            # 입력이 배치가 아닐 경우(1차원일 경우) 아래 식이 작동하지 않아 그냥 대충 2차원으로 때운다\n",
    "    t = t.reshape(1, t.size)\n",
    "    y = y.reshape(1, y.size)\n",
    "  \n",
    "  # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "  if t.size == y.size:\n",
    "    t = t.argmax(axis=1)\n",
    "            \n",
    "  batch_size = y.shape[0]\n",
    "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "t = np.array([\n",
    "  [0, 0, 1, 0, 0],\n",
    "  [0, 0, 1, 0, 0],\n",
    "  [0, 0, 1, 0, 0] # 정답이 2일때\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "  [0.4, 0.1, 0.2, 0.05, 0.2], # 정답을 0으로 예측 (틀림)\n",
    "  [0.4, 0.1, 0.2, 0.05, 0.2], # 정답을 0으로 예측 (틀림)\n",
    "  [0.2, 0.1, 0.6, 0.05, 0.2]  # 정답을 2으로 예측 (정답)\n",
    "])\n",
    "\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8770294422109671)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([\n",
    "  [0.4, 0.1, 0.2, 0.05, 0.2], # 정답을 0으로 예측 (틀림)\n",
    "  [0.2, 0.1, 0.6, 0.05, 0.2], # 정답을 2으로 예측 (정답)\n",
    "  [0.2, 0.1, 0.6, 0.05, 0.2]  # 정답을 2으로 예측 (정답)\n",
    "])\n",
    "\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치\n",
    "손실 함수를 사용하려면 먼저 예측값을 알아야하므로 순전파를 완료해야 한다. \\\n",
    "게다가 완료한 순전파 값들을 계산하여 손실률을 구해야한다\n",
    "\n",
    "위 예제에서는 순전파 완료 값을 직접 지정했고 개수도 몇개 안되므로 빠르게 끝났지만\\\n",
    "데이터셋이 매우 많을 경우를 생각해보면 손실 함수 실행 속도가 너무 느릴 수 있다.\n",
    "\n",
    "그래서 데이터셋에서 일부분을 랜덤하게 뽑아와 그거의 대한 손실 함수만 계산한 하는 것을 미니배치라고 하며\\\n",
    "이때 이 미니배치의 크기를 **배치 사이즈**라고 부른다. 또한 이 미니배치를 여러번 반복해서 데이터 셋을 완전히 1번 소진하였을때 1 **에폭**이라 부르며\\\n",
    "이 에폭값이 높으면 높을수록 데이터셋을 여러번 학습에 사용했다는 것을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정답률이 아니라 손실율을 쓰는 이유?\n",
    "정답률을 기준으로 하면 되지 왜 손실율을 쓰는지는 미분의 정의와 관련이 있다.\n",
    "\n",
    "미분은 함수 f의 그래프가 있을때 한없이 작은 범위에서의 기울기를 구하는 것을 의미한다.\\\n",
    "미분을 통해 자신의 위치의 기울기를 알게 되면 우리가 원하는 값으로 가는 기울기를 제시해 줄 수 있다는 것이다.\n",
    "\n",
    "이것을 우리가 배우는 것에 대입하면 퍼셉트론의 w, b에 대한 손실함수를 미분하여 손실율이 가장 적도록 만드는 기울기를 알 수 있다는 것이다. \\\n",
    "그리고 그 기울기를 적절히 타고 내려가면 손실율이 낮은 w와 b값을 알 수 있게 된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수의 대한 미분 (=특정 지점 기울기 구하기)\n",
    "위에서 설명한 미분 함수는 다음과 같이 정의할 수 있다.\n",
    "\n",
    "미분을 계산할 함수 f를 받으며 특정 지점 좌표인 numpy 배열 x를 받는다.\\\n",
    "함수의 파라미터가 많으므로 편미분이라 할 수 있으며 폰노이만식 컴퓨터의 소숫점 계산 문제를 해결하기 위해 차분을 사용하도록 약간 변형되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x: np.ndarray):\n",
    "  h = 1e-4\n",
    "  grad = np.zeros_like(x)\n",
    "\n",
    "  for idx in range(x.size):\n",
    "    tmp = x[idx]\n",
    "\n",
    "    x[idx] = tmp + h\n",
    "    fxh1 = f(x)\n",
    "\n",
    "    x[idx] = tmp - h\n",
    "    fxh2 = f(x)\n",
    "\n",
    "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "    x[idx] = tmp\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 이제 함수 f 을 미분해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x: np.ndarray):\n",
    "  return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "numerical_gradient(f, np.array([3.0, 4.0])) # 출력시 numpy가 결과값을 약간 반올림 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(f, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(f, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치를 고려한 함수는 다음과 같이 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient_no_batch(f, x):\n",
    "  h = 1e-4\n",
    "  grad = np.zeros_like(x)\n",
    "\n",
    "  for idx in range(x.size):\n",
    "    tmp = x[idx]\n",
    "\n",
    "    x[idx] = tmp + h\n",
    "    fxh1 = f(x)\n",
    "\n",
    "    x[idx] = tmp - h\n",
    "    fxh2 = f(x)\n",
    "\n",
    "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "    x[idx] = tmp\n",
    "\n",
    "  return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    \n",
    "    grad = np.zeros_like(X)\n",
    "    for idx, x in enumerate(X):\n",
    "        grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법\n",
    "경사 하강법은 간단하게 말하면 현재 위치를 하나 잡고 미분한뒤 그 기울기 만큼 조금 이동 후 다시 미분.. 을 반복하는 것을 의미한다.\n",
    "\n",
    "이때 여러가지 인자값이 필요하게 되는데:\n",
    "\n",
    "`f`: 경사하강법을 시행할 함수\\\n",
    "`init_x`: 시작 지점 좌표\\\n",
    "`lr`: learning rate라고도 하며 미분을 통해 알아낸 기울기를 통해 얼마만큼 이동할 것인가를 의미한다.\\\n",
    "`step_num`: 미분의 기울기와 lr값을 통해 몇번이나 내려가는 작업을 반복할꺼냐는 의미이다.\n",
    "\n",
    "이때 lr이 너무 크게 되면 이리저리 왔다갔다 하느라 최저점을 찾기 어려워지고\\\n",
    "lr이 너무 작게 되면 하강에 너무 오래걸려 별로 움직이지를 않는다.\n",
    "\n",
    "코드로 작성하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x: np.ndarray, lr=0.01, step_num=100):\n",
    "  x = init_x\n",
    "\n",
    "  for _ in range(step_num):\n",
    "    grad = numerical_gradient(f, x)\n",
    "    x -= lr * grad\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(f, init_x, lr=10.0, step_num=100) # lr이 너무 클때 최저점 못찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(f, init_x, lr=1e-10, step_num=100) # lr이 너무 작을때 별로 움직이지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(f, init_x, lr=0.1, step_num=100) # 적정값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엑조디아\n",
    "다 합쳐보자, 2층짜리 신경망이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeModel:\n",
    "  # 초기값 설정\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    self.params = {}\n",
    "    self.params['W1'] = 0.01 * np.random.randn(input_size, hidden_size) # 초기 weight 값은 그냥 랜덤하게 해보자, 랜덤이 너무 커서 0.01을 곱해줬다\n",
    "    self.params['b1'] = np.zeros(hidden_size)                           # bias값은 비운다\n",
    "    \n",
    "    self.params['W2'] = 0.01 * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "  # 예측 (순전파)\n",
    "  def predict(self, x):\n",
    "    W1, W2 = self.params['W1'], self.params['W2']\n",
    "    b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1   # 퍼셉트론 실행\n",
    "    z1 = sigmoid(a1)          # 활성화 함수 실행\n",
    "\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)           # n>2개 classficiation 문제이므로 softmax 사용\n",
    "\n",
    "    return y\n",
    "  \n",
    "  # 손실 함수\n",
    "  def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "\n",
    "    return cross_entropy_error(y, t)\n",
    "\n",
    "  # 정확도 테스트 함수\n",
    "  def accuracy(self, x, t):\n",
    "    y = self.predict(x)\n",
    "\n",
    "    y = np.argmax(y, axis=1) # 예측에서 softmax 한 것중 가장 큰것 선택\n",
    "    t = np.argmax(t, axis=1) # 정답 데이터셋은 one hot 인코딩 되어 있어서 가장 큰것 선택\n",
    "\n",
    "    return np.sum(y == t) / float(x.shape[0])\n",
    "  \n",
    "  # 손실 함수의 대한 기울기 계산 (미분)\n",
    "  def numerical_gradient(self, x, t):\n",
    "    loss_W = lambda W: self.loss(x, t) # 미분 대상 함수 (손실 함수)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, self.params['W1']) # 1층 w값의 대한 기울기\n",
    "    grads['b1'] = numerical_gradient(loss_W, self.params['b1']) # 1층 b값의 대한 기울기\n",
    "\n",
    "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 위 신경망을 MNIST 데이터셋으로 학습시켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "\n",
    "# \n",
    "# 네트워크 준비 ---\n",
    "net = MyAwesomeModel(\n",
    "  input_size  = 28 * 28, # MNIST의 사진 크기가 28 픽셀 곱하기 28이므로 입력층 28*28노드\n",
    "  hidden_size = 50,     # 은닉층 퍼셉트론 100개\n",
    "  output_size = 10       # 0부터 10까지를 예측하는 classification 문제이므로 출력층 퍼셉트론 10개 \n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "# 데이터셋 준비 --- \n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "  mnist_loader.load_dataset(normalize=True, one_hot_label=True)\n",
    "\n",
    "\n",
    "#\n",
    "# 하이퍼파라미터 ---\n",
    "\n",
    "epochs        = 16                 # 데이터셋 전체 학습 횟수\n",
    "train_size    = x_train.shape[0]   # 총 데이터셋의 개수\n",
    "batch_size    = 100                # 미니배치 크기\n",
    "learning_rate = 0.1                # 경사하강법 lr\n",
    "\n",
    "\n",
    "#\n",
    "# 학습 시작\n",
    "\n",
    "# 한 에폭 시작\n",
    "for epoch_idx in range(epochs):\n",
    "  print(f\"Epoch #{epoch_idx} ---\")\n",
    "\n",
    "  # 데이터셋 섞기 (데이터셋이 혹시나 제대로 분포되어 있지 않을 수 있어 한번 섞어준다. ex: 앞에는 정답이 0인 사진만 있는다던지...)\n",
    "  s = np.arange(0, train_size, 1)\n",
    "  np.random.shuffle(s)\n",
    "\n",
    "  x_train = x_train[s]\n",
    "  t_train = t_train[s]\n",
    "\n",
    "  # 한 미니배치 생성\n",
    "  for iter_start in range(0, train_size, batch_size):\n",
    "    print(f\"  Mini-Batch: {iter_start}...{iter_start+batch_size}\")\n",
    "\n",
    "    x_batch = x_train[iter_start:iter_start+batch_size]\n",
    "    t_batch = t_train[iter_start:iter_start+batch_size]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 기울기 반영\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "      net.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 손실율 기록\n",
    "    loss = net.loss(x_batch, t_batch)\n",
    "    print(f\"  Loss: {loss}\")\n",
    "\n",
    "  # 에폭 종료후 정확도 계산\n",
    "  train_acc = net.accuracy(x_train, t_train)\n",
    "  test_acc = net.accuracy(x_test, t_test)\n",
    "\n",
    "  print(f\"Epoch #{epoch_idx} accuracy: train {train_acc}, test {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 학습 코드를 실행시키면 우리가 작성한 미분 함수로는 매우 학습에 오래 걸린다는 것을 알 수 있다.\n",
    "\n",
    "이 알고리즘을 쫌 더 개선해 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차역전파법 (역전파)\n",
    "앞의 수치 미분을 통해 가중치를 수정하는 방법은 구현이 간단하고 이해하기 쉽지만 시간이 오래 걸린다\\\n",
    "이번에는 효율적으로 기울기를 계산하는 오차역전파법을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산 그래프\n",
    "다음과 같은 간단한 함수 f가 있다고 가정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x: int):\n",
    "  return x * 12 + 30\n",
    "\n",
    "f(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수는 x로 10을 주었을때 150을 반환한다.\\\n",
    "이 함수를 계산 그래프를 통해 나타내면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"494pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 494.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 490,-112 490,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;a -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.42,-90C76.44,-90 107.63,-90 132.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.16,-93.5 142.16,-90 132.16,-86.5 132.16,-93.5\"/>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M198.42,-90C220.44,-90 251.63,-90 276.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276.16,-93.5 286.16,-90 276.16,-86.5 276.16,-93.5\"/>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">f(x)</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.42,-90C364.44,-90 395.63,-90 420.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.16,-93.5 430.16,-90 420.16,-86.5 420.16,-93.5\"/>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<!-- a1&#45;&gt;a -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>a1&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.27,-33.27C124.01,-43.01 136.81,-55.81 147.79,-66.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"145.14,-69.09 154.68,-73.68 150.09,-64.14 145.14,-69.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"124.28\" y=\"-50.98\" font-family=\"Times,serif\" font-size=\"14.00\">12</text>\n",
       "</g>\n",
       "<!-- b1 -->\n",
       "<!-- b1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>b1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.27,-33.27C268.01,-43.01 280.81,-55.81 291.79,-66.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"289.14,-69.09 298.68,-73.68 294.09,-64.14 289.14,-69.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.28\" y=\"-50.98\" font-family=\"Times,serif\" font-size=\"14.00\">30</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2a81d790>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "dot = graphviz.Digraph(\n",
    "  strict=True,\n",
    "  engine='fdp'\n",
    ")\n",
    "\n",
    "dot.node('x',         pos='0,0!')\n",
    "dot.node('a', '*',    pos='2,0!')\n",
    "dot.node('b', '+',    pos='4,0!')\n",
    "dot.node('c', 'f(x)', pos='6,0!')\n",
    "\n",
    "dot.node('a1', style='invis', pos='1,-1!')\n",
    "dot.node('b1', style='invis', pos='3,-1!')\n",
    "\n",
    "dot.edges(['xa', 'ab', 'bc'])\n",
    "dot.edge('a1', 'a', '12')\n",
    "dot.edge('b1', 'b', '30')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x에 10을 대입하였을때는 다음과 같이 그릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"494pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 494.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 490,-112 490,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;a -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.42,-90C76.44,-90 107.63,-90 132.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.16,-93.5 142.16,-90 132.16,-86.5 132.16,-93.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"86.57\" y=\"-90.95\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M198.42,-90C220.44,-90 251.63,-90 276.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276.16,-93.5 286.16,-90 276.16,-86.5 276.16,-93.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.19\" y=\"-90.95\" font-family=\"Times,serif\" font-size=\"14.00\">120</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.42,-90C364.44,-90 395.63,-90 420.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.16,-93.5 430.16,-90 420.16,-86.5 420.16,-93.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"371.19\" y=\"-90.95\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<!-- a1&#45;&gt;a -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>a1&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.27,-33.27C124.01,-43.01 136.81,-55.81 147.79,-66.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"145.14,-69.09 154.68,-73.68 150.09,-64.14 145.14,-69.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"124.28\" y=\"-50.98\" font-family=\"Times,serif\" font-size=\"14.00\">12</text>\n",
       "</g>\n",
       "<!-- b1 -->\n",
       "<!-- b1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>b1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.27,-33.27C268.01,-43.01 280.81,-55.81 291.79,-66.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"289.14,-69.09 298.68,-73.68 294.09,-64.14 289.14,-69.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.28\" y=\"-50.98\" font-family=\"Times,serif\" font-size=\"14.00\">30</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2a81d790>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.node('x', '10')\n",
    "dot.node('c', '150')\n",
    "\n",
    "dot.edge('x', 'a', '10')\n",
    "dot.edge('a', 'b', '120')\n",
    "dot.edge('b', 'c', '150')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 그럼 미분을 통해 결과값 150의 대한 각 연산들의 기울기 (결과값에 영향을 미치는 정도)를 알아보자\n",
    "\n",
    "덧셈의 경우 두 값이 상승하는 만큼 결과값도 상승할 것이므로 다음과 같이 표시할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"494pt\" height=\"118pt\"\n",
       " viewBox=\"0.00 0.00 494.00 117.89\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 113.89)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-113.89 490,-113.89 490,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;a -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M54.42,-90C76.84,-90 108.77,-90 133.55,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"133.22,-93.5 143.22,-90 133.22,-86.5 133.22,-93.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.24\" y=\"-90.95\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M197.08,-95.34C219.91,-96.99 253.33,-97.19 278.76,-95.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"278.89,-99.45 288.66,-95.35 278.47,-92.46 278.89,-99.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.79\" y=\"-96.59\" font-family=\"Times,serif\" font-size=\"14.00\">120</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;a -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>b&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.92,-84.66C266.39,-83.03 233.57,-82.81 208.27,-84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.17,-80.5 198.39,-84.58 208.58,-87.49 208.17,-80.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"245.22\" y=\"-85.28\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M341.08,-95.34C363.91,-96.99 397.33,-97.19 422.76,-95.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"422.89,-99.45 432.66,-95.35 422.47,-92.46 422.89,-99.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"371.79\" y=\"-96.59\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- b1 -->\n",
       "<!-- b&#45;&gt;b1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>b&#45;&gt;b1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.59,-73C296.95,-62.43 283.92,-48.9 271.94,-37.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"274.42,-35.37 264.62,-31.34 269.76,-40.6 274.42,-35.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.39\" y=\"-56.38\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;b -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>c&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M432.92,-84.66C410.39,-83.03 377.57,-82.81 352.27,-84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.17,-80.5 342.39,-84.58 352.58,-87.49 352.17,-80.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"389.22\" y=\"-85.28\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<!-- a1&#45;&gt;a -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>a1&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M114.27,-33.27C124.2,-43.2 137.32,-56.32 148.44,-67.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"145.89,-69.84 155.43,-74.43 150.84,-64.89 145.89,-69.84\"/>\n",
       "<text text-anchor=\"middle\" x=\"124.6\" y=\"-51.3\" font-family=\"Times,serif\" font-size=\"14.00\">12</text>\n",
       "</g>\n",
       "<!-- b1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>b1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M252.41,-35C261.22,-45.78 274.59,-59.63 286.75,-70.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"284.37,-73.35 294.17,-77.37 289.01,-68.12 284.37,-73.35\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.83\" y=\"-53.84\" font-family=\"Times,serif\" font-size=\"14.00\">30</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2a81d790>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.edge('c', 'b', '1')\n",
    "\n",
    "dot.edge('b', 'a', '1')\n",
    "dot.edge('b', 'b1', '1')\n",
    "\n",
    "dot.edge('x', 'a', '10',  penwidth='0.3')\n",
    "dot.edge('a', 'b', '120', penwidth='0.3')\n",
    "dot.edge('b', 'c', '150', penwidth='0.3')\n",
    "dot.edge('a1', 'a', '12', penwidth='0.3')\n",
    "dot.edge('b1', 'b', '30', penwidth='0.3')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "곱셈의 경우 하나의 값을 변경하였을때 반대편 값 만큼 곱해져서 영향이 발생할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"494pt\" height=\"118pt\"\n",
       " viewBox=\"0.00 0.00 494.00 117.89\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 113.89)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-113.89 490,-113.89 490,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;a -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M53.08,-95.34C75.91,-96.99 109.33,-97.19 134.76,-95.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"134.89,-99.45 144.66,-95.35 134.47,-92.46 134.89,-99.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.17\" y=\"-96.59\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;x -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>a&#45;&gt;x</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.92,-84.66C122.39,-83.03 89.57,-82.81 64.27,-84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.17,-80.5 54.39,-84.58 64.58,-87.49 64.17,-80.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.84\" y=\"-71.03\" font-family=\"Times,serif\" font-size=\"14.00\">12</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M197.08,-95.34C219.91,-96.99 253.33,-97.19 278.76,-95.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"278.89,-99.45 288.66,-95.35 278.47,-92.46 278.89,-99.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.79\" y=\"-96.59\" font-family=\"Times,serif\" font-size=\"14.00\">120</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<!-- a&#45;&gt;a1 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>a&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.59,-73C152.95,-62.43 139.92,-48.9 127.94,-37.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"130.42,-35.37 120.62,-31.34 125.76,-40.6 130.42,-35.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.02\" y=\"-56.38\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;a -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>b&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.92,-84.66C266.39,-83.03 233.57,-82.81 208.27,-84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.17,-80.5 198.39,-84.58 208.58,-87.49 208.17,-80.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"245.22\" y=\"-85.28\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M341.08,-95.34C363.91,-96.99 397.33,-97.19 422.76,-95.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"422.89,-99.45 432.66,-95.35 422.47,-92.46 422.89,-99.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"371.79\" y=\"-96.59\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- b1 -->\n",
       "<!-- b&#45;&gt;b1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>b&#45;&gt;b1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.59,-73C296.95,-62.43 283.92,-48.9 271.94,-37.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"274.42,-35.37 264.62,-31.34 269.76,-40.6 274.42,-35.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.39\" y=\"-56.38\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;b -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>c&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M432.92,-84.66C410.39,-83.03 377.57,-82.81 352.27,-84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.17,-80.5 342.39,-84.58 352.58,-87.49 352.17,-80.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"389.22\" y=\"-85.28\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;a -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>a1&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M108.41,-35C117.22,-45.78 130.59,-59.63 142.75,-70.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"140.37,-73.35 150.17,-77.37 145.01,-68.12 140.37,-73.35\"/>\n",
       "<text text-anchor=\"middle\" x=\"118.83\" y=\"-53.84\" font-family=\"Times,serif\" font-size=\"14.00\">12</text>\n",
       "</g>\n",
       "<!-- b1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>b1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M252.41,-35C261.22,-45.78 274.59,-59.63 286.75,-70.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"284.37,-73.35 294.17,-77.37 289.01,-68.12 284.37,-73.35\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.83\" y=\"-53.84\" font-family=\"Times,serif\" font-size=\"14.00\">30</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2a81d790>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.edge('a', 'x', '12')\n",
    "dot.edge('a', 'a1', '10')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 함수도 비슷한 방식으로 계산 그래프를 해석해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "  return ((x1 * 2) + (x2 * 3)) * 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"652pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 651.71 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 647.71,-256 647.71,4 -4,4\"/>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-155.82\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-155.82\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.42,-162C76.44,-162 107.63,-162 132.21,-162\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.16,-165.5 142.16,-162 132.16,-158.5 132.16,-165.5\"/>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">x2</text>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;a2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.42,-90C76.44,-90 107.63,-90 132.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.16,-93.5 142.16,-90 132.16,-86.5 132.16,-93.5\"/>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>a1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.74,-155.56C219.47,-149.88 252.87,-141.53 278.38,-135.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.16,-138.57 288.01,-132.75 277.46,-131.78 279.16,-138.57\"/>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;b -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>a2&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.74,-96.44C219.47,-102.12 252.87,-110.47 278.38,-116.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.46,-120.22 288.01,-119.25 279.16,-113.43 277.46,-120.22\"/>\n",
       "</g>\n",
       "<!-- x1a -->\n",
       "<!-- x1a&#45;&gt;a1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x1a&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.87,-223.06C72.9,-211.05 111.73,-191.64 139.14,-177.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.66,-181.08 148.04,-173.48 137.53,-174.82 140.66,-181.08\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.63\" y=\"-201.45\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- x2a -->\n",
       "<!-- x2a&#45;&gt;a2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x2a&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.87,-28.94C72.9,-40.95 111.73,-60.36 139.14,-74.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.53,-77.18 148.04,-78.52 140.66,-70.92 137.53,-77.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.63\" y=\"-52.45\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.42,-126C364.44,-126 395.63,-126 420.21,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.16,-129.5 430.16,-126 420.16,-122.5 420.16,-129.5\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"603\" cy=\"-126\" rx=\"40.71\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"603\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">f(x1, x2)</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;y -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>c&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M486.42,-126C504.55,-126 528.88,-126 550.63,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"550.54,-129.5 560.54,-126 550.54,-122.5 550.54,-129.5\"/>\n",
       "</g>\n",
       "<!-- cb -->\n",
       "<!-- cb&#45;&gt;c -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>cb&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.87,-64.94C360.9,-76.95 399.73,-96.36 427.14,-110.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"425.53,-113.18 436.04,-114.52 428.66,-106.92 425.53,-113.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"373.38\" y=\"-88.45\" font-family=\"Times,serif\" font-size=\"14.00\">1.1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2cb9fb00>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = graphviz.Digraph(\n",
    "  strict=True,\n",
    "  engine='fdp'\n",
    ")\n",
    "\n",
    "dot.node('x1', pos='0,1!')\n",
    "dot.node('x2', pos='0,0!')\n",
    "\n",
    "dot.node('a1', '*', pos='2,1!')\n",
    "dot.node('a2', '*', pos='2,0!')\n",
    "\n",
    "dot.node('x1a', style='invis', pos='0,2!')\n",
    "dot.node('x2a', style='invis', pos='0,-1!')\n",
    "\n",
    "dot.edge('x1', 'a1')\n",
    "dot.edge('x1a', 'a1', '2')\n",
    "\n",
    "dot.edge('x2', 'a2')\n",
    "dot.edge('x2a', 'a2', '3')\n",
    "\n",
    "dot.node('b', '+', pos='4,0.5!')\n",
    "\n",
    "dot.edge('a1', 'b')\n",
    "dot.edge('a2', 'b')\n",
    "\n",
    "dot.node('c', '*', pos='6,0.5!')\n",
    "dot.edge('b', 'c')\n",
    "\n",
    "dot.node('cb', style='invis', pos='4,-0.5!')\n",
    "dot.edge('cb', 'c', '1.1')\n",
    "\n",
    "dot.node('y', 'f(x1, x2)', pos='8,0.5!')\n",
    "dot.edge('c', 'y')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f에 `f(100, 150)` 를 넣었다고 가정했을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"638pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 638.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 634,-256 634,4 -4,4\"/>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-155.82\" font-family=\"Times,serif\" font-size=\"14.00\">100</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-155.82\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.42,-162C76.44,-162 107.63,-162 132.21,-162\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.16,-165.5 142.16,-162 132.16,-158.5 132.16,-165.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.19\" y=\"-162.95\" font-family=\"Times,serif\" font-size=\"14.00\">100</text>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;a2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.42,-90C76.44,-90 107.63,-90 132.21,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.16,-93.5 142.16,-90 132.16,-86.5 132.16,-93.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.19\" y=\"-90.95\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>a1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.74,-155.56C219.47,-149.88 252.87,-141.53 278.38,-135.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.16,-138.57 288.01,-132.75 277.46,-131.78 279.16,-138.57\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.44\" y=\"-146.31\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;b -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>a2&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.74,-96.44C219.47,-102.12 252.87,-110.47 278.38,-116.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.46,-120.22 288.01,-119.25 279.16,-113.43 277.46,-120.22\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.44\" y=\"-107.59\" font-family=\"Times,serif\" font-size=\"14.00\">450</text>\n",
       "</g>\n",
       "<!-- x1a -->\n",
       "<!-- x1a&#45;&gt;a1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x1a&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.87,-223.06C72.9,-211.05 111.73,-191.64 139.14,-177.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.66,-181.08 148.04,-173.48 137.53,-174.82 140.66,-181.08\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.63\" y=\"-201.45\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- x2a -->\n",
       "<!-- x2a&#45;&gt;a2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x2a&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.87,-28.94C72.9,-40.95 111.73,-60.36 139.14,-74.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.53,-77.18 148.04,-78.52 140.66,-70.92 137.53,-77.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.63\" y=\"-52.45\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.42,-126C364.44,-126 395.63,-126 420.21,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.16,-129.5 430.16,-126 420.16,-122.5 420.16,-129.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"371.19\" y=\"-126.95\" font-family=\"Times,serif\" font-size=\"14.00\">650</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"603\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"603\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">715</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;y -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>c&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M486.42,-126C508.44,-126 539.63,-126 564.21,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"564.16,-129.5 574.16,-126 564.16,-122.5 564.16,-129.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"515.19\" y=\"-126.95\" font-family=\"Times,serif\" font-size=\"14.00\">715</text>\n",
       "</g>\n",
       "<!-- cb -->\n",
       "<!-- cb&#45;&gt;c -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>cb&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.87,-64.94C360.9,-76.95 399.73,-96.36 427.14,-110.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"425.53,-113.18 436.04,-114.52 428.66,-106.92 425.53,-113.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"373.38\" y=\"-88.45\" font-family=\"Times,serif\" font-size=\"14.00\">1.1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2cb9fb00>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.node('x1', '100', pos='0,1!')\n",
    "dot.node('x2', '150', pos='0,0!')\n",
    "\n",
    "dot.edge('x1', 'a1', '100')\n",
    "dot.edge('x2', 'a2', '150')\n",
    "\n",
    "dot.edge('a1', 'b', '200')\n",
    "dot.edge('a2', 'b', '450')\n",
    "\n",
    "dot.edge('b', 'c', '650')\n",
    "dot.edge('c', 'y', '715')\n",
    "\n",
    "dot.node('y', '715', pos='8,0.5!')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역방향으로 기울기를 표현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"638pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 638.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 634,-256 634,4 -4,4\"/>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-155.82\" font-family=\"Times,serif\" font-size=\"14.00\">100</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-155.82\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M53.08,-167.34C75.91,-168.99 109.33,-169.19 134.76,-167.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"134.89,-171.45 144.66,-167.35 134.47,-164.46 134.89,-171.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.79\" y=\"-168.59\" font-family=\"Times,serif\" font-size=\"14.00\">100</text>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-83.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;a2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M53.08,-95.34C75.91,-96.99 109.33,-97.19 134.76,-95.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"134.89,-99.45 144.66,-95.35 134.47,-92.46 134.89,-99.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.79\" y=\"-96.59\" font-family=\"Times,serif\" font-size=\"14.00\">150</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;x1 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>a1&#45;&gt;x1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.92,-156.66C122.39,-155.03 89.57,-154.81 64.27,-156\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.17,-152.5 54.39,-156.58 64.58,-159.49 64.17,-152.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.97\" y=\"-143.03\" font-family=\"Times,serif\" font-size=\"14.00\">2.2</text>\n",
       "</g>\n",
       "<!-- x1a -->\n",
       "<!-- a1&#45;&gt;x1a -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>a1&#45;&gt;x1a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M145.58,-168.99C119.83,-179.61 80.22,-199.25 53.94,-214.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.38,-211.16 45.55,-219.26 55.94,-217.19 52.38,-211.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.88\" y=\"-192.6\" font-family=\"Times,serif\" font-size=\"14.00\">110</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>a1&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M198.37,-160.66C222.44,-156.41 257.47,-147.81 282.86,-140.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"283.85,-143.38 292.32,-137 281.73,-136.71 283.85,-143.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.49\" y=\"-151.29\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;x2 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>a2&#45;&gt;x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.92,-84.66C122.39,-83.03 89.57,-82.81 64.27,-84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.17,-80.5 54.39,-84.58 64.58,-87.49 64.17,-80.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.97\" y=\"-71.03\" font-family=\"Times,serif\" font-size=\"14.00\">3.3</text>\n",
       "</g>\n",
       "<!-- x2a -->\n",
       "<!-- a2&#45;&gt;x2a -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>a2&#45;&gt;x2a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.43,-75.83C130.99,-62.15 91.68,-42.14 62.82,-29.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.29,-26.26 53.72,-25.55 61.54,-32.7 64.29,-26.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"98\" y=\"-39.33\" font-family=\"Times,serif\" font-size=\"14.00\">165</text>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;b -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>a2&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M193.21,-100.85C215.63,-108.35 250.66,-117.38 277.41,-122.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"276.65,-126.15 287.13,-124.57 277.95,-119.27 276.65,-126.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"225.18\" y=\"-112.74\" font-family=\"Times,serif\" font-size=\"14.00\">450</text>\n",
       "</g>\n",
       "<!-- x1a&#45;&gt;a1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x1a&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M52.42,-227.01C78.51,-216.25 118.84,-196.23 145.1,-181.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"146.54,-184.31 153.36,-176.2 142.98,-178.29 146.54,-184.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.39\" y=\"-205\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- x2a&#45;&gt;a2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x2a&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M44.57,-32.17C67.31,-46.03 107.36,-66.39 136.31,-79.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"134.68,-82.17 145.25,-82.87 137.42,-75.73 134.68,-82.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.07\" y=\"-56.57\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;a1 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>b&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287.63,-127.34C263.88,-131.54 229.46,-139.96 204.16,-147.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.16,-144.31 194.69,-150.68 205.28,-150.98 203.16,-144.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"254.52\" y=\"-138.45\" font-family=\"Times,serif\" font-size=\"14.00\">1.1</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;a2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>b&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.79,-115.15C270.67,-107.75 236.27,-98.86 209.66,-93.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"210.39,-90.06 199.91,-91.63 209.08,-96.94 210.39,-90.06\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.6\" y=\"-91.02\" font-family=\"Times,serif\" font-size=\"14.00\">1.1</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M341.08,-131.34C363.91,-132.99 397.33,-133.19 422.76,-131.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"422.89,-135.45 432.66,-131.35 422.47,-128.46 422.89,-135.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"371.79\" y=\"-132.59\" font-family=\"Times,serif\" font-size=\"14.00\">650</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;b -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>c&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M432.92,-120.66C410.39,-119.03 377.57,-118.81 352.27,-120\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.17,-116.5 342.39,-120.58 352.58,-123.49 352.17,-116.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"383.97\" y=\"-107.03\" font-family=\"Times,serif\" font-size=\"14.00\">1.1</text>\n",
       "</g>\n",
       "<!-- cb -->\n",
       "<!-- c&#45;&gt;cb -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>c&#45;&gt;cb</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M441.43,-111.83C418.99,-98.15 379.68,-78.14 350.82,-65.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.29,-62.26 341.72,-61.55 349.54,-68.7 352.29,-62.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"386\" y=\"-75.33\" font-family=\"Times,serif\" font-size=\"14.00\">650</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"603\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"603\" y=\"-119.83\" font-family=\"Times,serif\" font-size=\"14.00\">715</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;y -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>c&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M485.08,-131.34C507.91,-132.99 541.33,-133.19 566.76,-131.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"566.89,-135.45 576.66,-131.35 566.47,-128.46 566.89,-135.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"515.79\" y=\"-132.59\" font-family=\"Times,serif\" font-size=\"14.00\">715</text>\n",
       "</g>\n",
       "<!-- cb&#45;&gt;c -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>cb&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"0.3\" d=\"M332.57,-68.17C355.31,-82.03 395.36,-102.39 424.31,-115.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"0.3\" points=\"422.68,-118.17 433.25,-118.87 425.42,-111.73 422.68,-118.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"369.82\" y=\"-92.57\" font-family=\"Times,serif\" font-size=\"14.00\">1.1</text>\n",
       "</g>\n",
       "<!-- y&#45;&gt;c -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>y&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M576.92,-120.66C554.39,-119.03 521.57,-118.81 496.27,-120\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"496.17,-116.5 486.39,-120.58 496.58,-123.49 496.17,-116.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"533.22\" y=\"-121.28\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6f2cb9fb00>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.edge('x1', 'a1', '100', penwidth='0.3')\n",
    "dot.edge('x2', 'a2', '150', penwidth='0.3')\n",
    "dot.edge('a1', 'b', '200', penwidth='0.3')\n",
    "dot.edge('a2', 'b', '450', penwidth='0.3')\n",
    "dot.edge('b', 'c', '650', penwidth='0.3')\n",
    "dot.edge('c', 'y', '715', penwidth='0.3')\n",
    "dot.edge('x1a', 'a1', '2', penwidth='0.3')\n",
    "dot.edge('x2a', 'a2', '3', penwidth='0.3')\n",
    "dot.edge('cb', 'c', '1.1', penwidth='0.3')\n",
    "\n",
    "dot.edge('y', 'c', '1')\n",
    "dot.edge('c', 'b', '1.1')\n",
    "dot.edge('b', 'a2', '1.1')\n",
    "dot.edge('b', 'a1', '1.1')\n",
    "dot.edge('a2', 'x2', '3.3')\n",
    "dot.edge('a1', 'x1', '2.2')\n",
    "\n",
    "dot.edge('a1', 'x1a', '110')\n",
    "dot.edge('a2', 'x2a', '165')\n",
    "dot.edge('c', 'cb', '650')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런식으로 어떠한 함수가 있을 때 뒤에서 부터 미분을 하여\\\n",
    "결과값의 대한 기울기를 알아내는 것을 역전파라고 한다.\n",
    "\n",
    "코드로 나타내면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 곱셈 계층의 역전파\n",
    "곱셈 계층은 (순전파 기준으로) 다음층에서 받은 기울기에 곱할 두 값을 반대로 곱하여 기울기를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "  def __init__(self):\n",
    "    self.x = None\n",
    "    self.y = None\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    out = x * y\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    dx = dout * self.y  # x와 y를 바꾼다.\n",
    "    dy = dout * self.x\n",
    "\n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 덧셈 계층의 역전파\n",
    "덧셈 계층은 (순전파 기준으로) 다음층에서 받은 기울기를 그대로 전달하여 기울기를 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    out = x + y\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    dx = dout * 1\n",
    "    dy = dout * 1\n",
    "\n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현한 곱셈, 덧셈 계층을 활용하여 전의 f 함수를 구현하면 다음과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = 100\n",
    "x2 = 150\n",
    "\n",
    "layer1_1 = MulLayer()\n",
    "layer1_2 = MulLayer()\n",
    "\n",
    "const1_1 = 2\n",
    "const1_2 = 3\n",
    "\n",
    "out1_1 = layer1_1.forward(x1, const1_1)\n",
    "out1_2 = layer1_2.forward(x2, const1_2)\n",
    "\n",
    "layer2 = AddLayer()\n",
    "out2 = layer2.forward(out1_1, out1_2)\n",
    "\n",
    "layer3 = MulLayer()\n",
    "const3 = 1.1\n",
    "out3 = layer3.forward(out2, const3)\n",
    "\n",
    "int(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현한 계산 그래프에서 역전파를 수행해보자\\\n",
    "전에 그렸던 계산 그래프의 예상 역전파 output과 같은것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110.00000000000001, 2.2, 3.3000000000000003, 165.0, 1.1, 1.1, 1.1, 650, 1]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dout3 = 1\n",
    "\n",
    "dout2, dconst3 = layer3.backward(dout3)\n",
    "\n",
    "dout1_1, dout1_2 = layer2.backward(dout2)\n",
    "\n",
    "dx1, dconst1_1 = layer1_1.backward(dout1_1)\n",
    "dx2, dconst1_2 = layer1_2.backward(dout1_2)\n",
    "\n",
    "[dconst1_1, dx1, dx2, dconst1_2, dout1_1, dout1_2, dout2, dconst3, dout3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수들의 역전파\n",
    "이제 신경망의 함수들의 역전파를 구현해보자\n",
    "\n",
    "ReLU는 0보다 클 경우에만 그대로 순전파 하는 함수이다.\n",
    "\n",
    "역전파에도 마찬가지로 0보다 큰 경우에만 이전 레이어에 기울기를 그대로 전달하며\\\n",
    "0보다 크지 않은 경우에는 기울기를 0으로 만들어 전달되지 않도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "  def __init__(self):\n",
    "    self.mask = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.mask = (x <= 0)\n",
    "    out = x.copy()\n",
    "    out[self.mask] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    dout[self.mask] = 0\n",
    "    dx = dout\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 시그모이드 활성화 함수의 역전파를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "  def __init__(self):\n",
    "    self.out = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = sigmoid(x)\n",
    "    self.out = out\n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론 계층의 역전파\n",
    "퍼셉트론 계층, 즉 w를 곱하고 b를 더하는 계층, Affine 계층이라고도 불리는 이 계층은\\\n",
    "일반적인 곱셈 계층이 아니라 행렬의 점곱 계층이 존재한다.\n",
    "\n",
    "다음과 같이 파이썬으로 작성할 수 있으며 이때 행렬의 T값은 행렬을 90도 눕힌것이라고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "  def __init__(self, W, b):\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "    \n",
    "    self.x = None\n",
    "    self.original_x_shape = None\n",
    "    \n",
    "    # 가중치와 편향 매개변수의 미분\n",
    "    self.dW = None\n",
    "    self.db = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    # 텐서 대응\n",
    "    self.original_x_shape = x.shape\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    self.x = x\n",
    "\n",
    "    out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    dx = np.dot(dout, self.W.T)\n",
    "    self.dW = np.dot(self.x.T, dout)\n",
    "    self.db = np.sum(dout, axis=0)\n",
    "    \n",
    "    dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스-크로스 엔트로피 손실 함수 계층의 역전파\n",
    "소프트맥스와 크로스 엔트로피 손실 함수를 한꺼번에 하나의 클래스로 구현하면 다음과 같다.\n",
    "\n",
    "소프트맥스를 출력층으로 하는 신경망에서 크로스 엔트로피 손실 함수를 쓰는 이유는\\\n",
    "역전파를 하였을때 해당 계층 기울기가 깔끔하게 떨어지기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SoftmaxWithLoss:\n",
    "  def __init__(self):\n",
    "    self.loss = None # 손실함수\n",
    "    self.y = None    # softmax의 출력\n",
    "    self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "      \n",
    "  def forward(self, x, t):\n",
    "    self.t = t\n",
    "    self.y = softmax(x)\n",
    "    self.loss = cross_entropy_error(self.y, self.t)\n",
    "    \n",
    "    return self.loss\n",
    "\n",
    "  def backward(self, dout=1):\n",
    "    batch_size = self.t.shape[0]\n",
    "    if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "      dx = (self.y - self.t) / batch_size\n",
    "    else:\n",
    "      dx = self.y.copy()\n",
    "      dx[np.arange(batch_size), self.t] -= 1\n",
    "      dx = dx / batch_size\n",
    "    \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better 엑조디아\n",
    "\n",
    "이때까지 배운 오차역전파법 (역전파, 뒤에서 부터 미분하며 기울기를 구하는 것)를 통해 w와 b값을 조정하도록 하면\\\n",
    "수치 미분으로 구현했던 전의 네트워크 보다 학습 시간이 매우 단축될 수 있다. 자 다시 한번 합쳐보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class MyAwesomeModelV2:\n",
    "  # 초기값 설정\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    self.params = {}\n",
    "    self.params['W1'] = 0.01 * np.random.randn(input_size, hidden_size) # 초기 weight 값은 그냥 랜덤하게 해보자, 랜덤이 너무 커서 0.01을 곱해줬다\n",
    "    self.params['b1'] = np.zeros(hidden_size)                         # bias값은 비운다\n",
    "    \n",
    "    self.params['W2'] = 0.01 * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 레이어 쌓기 (추가됨)\n",
    "    self.layers = OrderedDict()\n",
    "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "    self.layers['Relu1']   = Relu()\n",
    "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "    self.lastLayer         = SoftmaxWithLoss()\n",
    "\n",
    "  # 예측 (순전파, 수정됨)\n",
    "  def predict(self, x):\n",
    "    for layer in self.layers.values():\n",
    "      x = layer.forward(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "  # 손실 함수 (수정됨)\n",
    "  def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "\n",
    "    return self.lastLayer.forward(y, t)\n",
    "\n",
    "  # 정확도 테스트 함수\n",
    "  def accuracy(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    y = np.argmax(y, axis=1)\n",
    "\n",
    "    if t.ndim != 1:\n",
    "      t = np.argmax(t, axis=1)\n",
    "\n",
    "    return np.sum(y == t) / float(x.shape[0])\n",
    "  \n",
    "  # 역전파를 통한 기울기 계산 (미분, 추가됨)\n",
    "  def gradient(self, x, t):\n",
    "    self.loss(x, t)\n",
    "\n",
    "    dout = 1\n",
    "    dout = self.lastLayer.backward(dout)\n",
    "\n",
    "    layers = list(self.layers.values())\n",
    "    layers.reverse()\n",
    "\n",
    "    for layer in layers:\n",
    "      dout = layer.backward(dout)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = self.layers['Affine1'].dW\n",
    "    grads['b1'] = self.layers['Affine1'].db\n",
    "    grads['W2'] = self.layers['Affine2'].dW\n",
    "    grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 다시 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 2.2964752151399024\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 2.2929821861493385\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 2.287743393605074\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 2.2787347077444937\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 2.2676468011195716\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 2.253648838779269\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 2.2341961637240058\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 2.201766070656175\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 2.151732652357577\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 2.0802692294412535\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 2.0065631547067055\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 1.9118949274369796\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 1.8334464856487325\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 1.7428438094409804\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 1.5436973438587787\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 1.4987072368371432\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 1.3862448063892325\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 1.487416011035132\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 1.172565517524953\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 1.0768331283674966\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.9645010441821823\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.9763570223319282\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.9290087832194556\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.9148943185239199\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.9335563885639387\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.7981549196418671\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.8292267232298138\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 1.0093086255630033\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.8691211923359302\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.8158979968599982\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.6825938031456802\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.7659099569484698\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.7028347806415616\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.6510599396533835\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.6685297379956687\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.7083641941029694\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.5581489068118651\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.5859599571880301\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.5201406539204558\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.58635843650003\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.5261818229297036\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.6962633022564763\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.7023517763679578\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.6870998677978858\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.5627251768299864\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.5956979544109491\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.6091537374184987\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.5623883951197531\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.5152142959024802\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.41758043436199727\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.4737329067496278\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.44813878011252956\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.4731224135019173\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.4411758846476615\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.44422639271254966\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.41456852902745006\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.4201874298413778\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.4827618516692711\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.35855910267600516\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.42300357353234325\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.396058807211702\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.35372809869455785\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.5196309992804679\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.5833629226673781\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.4810718929580947\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.47641265335559563\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.44287302000553747\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.5191574389597186\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.4068219200017652\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.39207970483832016\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.3623923836039695\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.34408861882459896\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.3046310336100076\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.3740473718802849\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.3854179864581555\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.39718738988259233\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.34983424029366744\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.42260559394187497\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.4307614620112086\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.3650901889325232\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.4108858250981785\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.428573704450009\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.37234408855815265\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.34788960991457996\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.34765059114933583\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.3689028377676583\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.33614099467531966\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.34934566328262634\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.3439869832456159\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.32401923025202\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.3250563338201836\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.3118142576607547\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.355147518510328\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.3527735430408337\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.3634108440219273\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.37938092833177894\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.296857432157048\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.2507776122449631\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.34346693964610125\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.4140982393947099\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.337213542360005\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.27896789144515566\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.3163974496905264\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.36842785356563207\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.30143609975883984\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.3285568068431213\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.32797814514335355\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.28867264608812687\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.2802836263177643\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.307185629919079\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.2872980338317871\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.29612879082970267\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.32048981209883887\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.25468390573955\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.2570212795177264\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.24435548648237845\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.2851674769911235\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.34737891550633354\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.2822275941389109\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.2837267594193842\n",
      "Epoch #0 accuracy: train 0.9091833333333333, test 0.9136\n",
      "Epoch #1 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.24386457417740587\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.26813689141839625\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.2425159720402134\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.26861577711727397\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.335577128266608\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.2890512534107741\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.2279587064088823\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.28457839835748794\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.25295059783565127\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.3209979217231149\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.3902304406822293\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.27106190931682145\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.2804011143400589\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.27052465437873785\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.3088103680373644\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.3232928848848936\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.26907832192183007\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.3110495254399207\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.2601533764243435\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.27937104131837975\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.30206263714853016\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.32861867211965146\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.2678546935715106\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.23560817345914342\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.2841073465296457\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.24963370521456604\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.32236345916358206\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.29695515959238217\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.2959058895927361\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.23704113919978398\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.28667849183505834\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.23200828604437906\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.2746754834643284\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.2811226409123167\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.2669249976695245\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.33022351406949246\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.2506717350298423\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.24735972844579635\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.2484565246502103\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.2723495359785615\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.30755469104729033\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.2724713261721872\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.2415269983802634\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.31100544886577913\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.2611462382395571\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.26821050532934704\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.2599357491335197\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.33800309664743405\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.2297026377309781\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.2947740236009513\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.25642691546098856\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.2745494697084892\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.29358270475000026\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.27445042826119154\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.19950644717957663\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.21361211572786976\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.23409378681655246\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.2572231913929742\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.28224984122003394\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.256298806790124\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.25362371736189615\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.18126986616069285\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.2061555447577757\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.2615259271078822\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.35293740417723735\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.27655052859058377\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.2421369210724738\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.2947242976606308\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.24604822812883417\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.20178207913672264\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.22572067575915342\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.2712198009508159\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.30429801794835204\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.22450916434349064\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.25751967679192467\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.2543824951303458\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.22056829802485664\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.16429827676874292\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.2932862242599733\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.21714378651741506\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.22279340641525786\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.22671953733417524\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.28540784183035767\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.2698602112025418\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.22084810260181065\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.20896681182577145\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.23520115952321075\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.20957021513633367\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.19171157930997879\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.2594977829780765\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.22362888181552973\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.24915729854666013\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.2208635383779474\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.2881623662482424\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.1969721547005899\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.2004242822306698\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.1785486500453045\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.26211899212889395\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.2628596014851592\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.18520224260843063\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.2681762742022216\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.23037395888398332\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.22913444953766782\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.2254645524363977\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.22666306140332698\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.20144346593800613\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.20815565587753454\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.17640579322037178\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.26919319036072875\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.1930070420427996\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.1882085424740249\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.21906104683899247\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.17062922823658927\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.26466022589032695\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.18574598496673145\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.19030243880568157\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.23849163444772525\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.20186776690513522\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.23173170628820713\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.22183011254270524\n",
      "Epoch #1 accuracy: train 0.9303, test 0.9322\n",
      "Epoch #2 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.2324761448545023\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.18767234631336482\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.19278369822257624\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.2524115967732448\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.21529098922049222\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.17566528486869823\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.24892200268879242\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.1873348408461294\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.2156385708135373\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.1936576132018061\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.1763172027303221\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.1779463742466963\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.20402488231329863\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.22656234619057777\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.23476805620974467\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.16580853377777532\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.2296078708861888\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.27139064456721657\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.20858301606827961\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.2500003570400941\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.19121509431701514\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.16210697590753947\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.16405695632162193\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.18766229488519104\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.22950678487475398\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.21260818781246235\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.22458002411489308\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.20323273214818932\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.15307636245808073\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.1906266597863681\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.18924497219476574\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.16971555732443785\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.25075049542483624\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.20593978627479556\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.20644384476905003\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.167222584222787\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.18398085378415566\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.18642261596414744\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.24417625153398087\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.22023646724354967\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.18617123352391474\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.16004951247027077\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.2081822180255416\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.20715903843400038\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.1917191801261729\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.24002448864205223\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.20415239232450397\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.2673143363048371\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.2518736740012766\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.25172794138661975\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.1912576912687733\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.2429517878688277\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.21605193180339108\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.19934675864254978\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.16961556646906717\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.1695081647753081\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.16898593136277157\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.2095649438895395\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.19444172825292358\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.1920105495087477\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.1590930148215597\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.17661545572091192\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.2428027554369881\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.1885026471573445\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.16405578483591937\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.15942990065160476\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.16524343684365111\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.16609709098685416\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.18952001735136378\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.1706904929692858\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.18690659990594222\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.1982822731149319\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.19340888657289287\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.21353729644592123\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.19074104142313578\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.23435648328676337\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.16726209557567517\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.21966307667559268\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.2208727255258646\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.20072155754068308\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.21229584245242036\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.14957086633748168\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.20622718908580595\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.1676027898891815\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.18557844697351009\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.21997894748962055\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.17336177905254932\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.1654761896529704\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.17435734855533022\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.17940781277148768\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.18286973704064893\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.15756341557078024\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.25325345224359536\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.12977247231704744\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.17370029770684375\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.1605418015237099\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.21625895797109007\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.19643957821097838\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.165818209470895\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.15370282172416003\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.17419150448782544\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.1583089435205696\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.16437857356833496\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.16212605114999906\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.19772879306991065\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.21460357088059845\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.1756189723089451\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.20891315983780231\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.2007043385700705\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.17487299650868893\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.20153283392466392\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.19830770537474643\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.17756065245236102\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.15312291726280944\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.18133246095449962\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.19225324821513523\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.20947338806067473\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.17332297641643613\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.17233405561552223\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.18636592711301803\n",
      "Epoch #2 accuracy: train 0.9398166666666666, test 0.9386\n",
      "Epoch #3 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.15904896770369675\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.14673845445330197\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.13894069029934386\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.13954576444840905\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.15846007859863598\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.15834637871596377\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.17258611303679397\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.18974902408198552\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.14867488831663384\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.2006525929060979\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.20214219090243193\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.16198386149137714\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.1387799291375502\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.15655418120125272\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.1487996833030682\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.18181144728596343\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.1276515741690765\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.1367235356426092\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.21914168248285906\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.1788576099940804\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.18730945926579967\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.1741114325844149\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.18080469274948865\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.17836131413315728\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.12090057249018131\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.15215847026349644\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.13240548521609866\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.13758571861939917\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.21205330850132623\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.20571842727778622\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.1501209388866975\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.16047054302386624\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.15812948096181406\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.22680205728977762\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.17938998250776172\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.1383941535770289\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.1376304493035996\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.23645171116953698\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.13857496070180042\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.12685729896397382\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.24624683356563293\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.14742864813427115\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.16153798568189898\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.1536149490918411\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.12600395749903628\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.1943789142453543\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.14200229460355598\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.14187114770815076\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.16491531233424153\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.1706218321665642\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.20516775672941853\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.1280482099109887\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.19263368043860293\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.13857109055691\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.1430335832178293\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.18057893659540525\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.16877787897387644\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.16257979186040775\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.17703761079053498\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.12621638498078808\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.15561249803035532\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.16960794684798638\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.18979877126248176\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.16628784958572385\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.13593662223817332\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.1414709081779459\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.11585700385548625\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.14974246679312886\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.19956673209955464\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.20189733640451055\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.1424773716471273\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.08787166780141148\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.14454590512409266\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.1465806812616562\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.11789729880198144\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.15250315595116032\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.14498033518447392\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.13928866064354964\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.1550471145497039\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.17466640083431395\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.15117870275184286\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.1301007340394495\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.20387310996226232\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.1851205058142445\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.18317138936931582\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.1418290473111917\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.11611289831967213\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.1541194296061074\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.1839121724875953\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.15968009734484656\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.13331248110677402\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.15639215744165444\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.10663287992265477\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.17920100935102518\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.14843853518975755\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.21810807831932055\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.17938387028150626\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.16577449967414887\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.18132946282706108\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.1849363711098578\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.17649410979266464\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.12851320514497494\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.16415481573733193\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.2237398615825637\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.16230007312103345\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.13111985693364522\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.12477616243082323\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.20686839322988518\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.13679471850341735\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.11479484302905649\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.14403585661805454\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.117018963852931\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.17063902041566584\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.15443625014528278\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.1498938206888401\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.1363111428393315\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.21189018811443905\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.16567354412682614\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.1860636611803048\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.1937531713321752\n",
      "Epoch #3 accuracy: train 0.953, test 0.9502\n",
      "Epoch #4 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.11442989316012483\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.12957255143286311\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.13292080137521842\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.13227371978050131\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.14078496168898588\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.14880226164985255\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.21880465258053095\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.1691866632489404\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.14541247358834342\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.15708874920210303\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.12870929937492098\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.12212237242263665\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.1728535529090957\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.128494852012086\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.17055092024786483\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.12187759947736974\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.13208816726498782\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.129239830760293\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.13499180346944478\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.14660515126176885\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.1729428169813898\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.14934423463972973\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.17513431812167504\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.12895175772108536\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.15103585904329023\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.15280784911110654\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.12686108641406602\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.16588628816283124\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.12045124312215284\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.17313074584284696\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.1181873100667996\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.13656834377102256\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.14319591588557773\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.1476453107892927\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.12218020020297365\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.16368983329365988\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.14173012728914192\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.11801120909648478\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.10123370078163199\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.12991606861601657\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.11104256576422741\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.12016322030054459\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.14354298011471336\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.15597105221510957\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.15682245580024287\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.12449679017933792\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.1469257156745691\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.13128236298513093\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.16963501622451788\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.12710386595756473\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.1423143328512407\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.12316076241156773\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.13286954810736754\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.1534563191372876\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.1302218224664306\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.14957841286698198\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.1354511312859983\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.10627865870405755\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.148293625857593\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.1305510531752752\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.1390670131873339\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.13338718021141305\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.13599992710042716\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.14671253668275971\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.13370214611479514\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.14510685867029718\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.12787875307324914\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.17066157732920748\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.12646328557498132\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.13309006733294737\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.13078014234550595\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.12532178894550752\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.10667157301947143\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.12925871719276827\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.12345755994541308\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.118684613925046\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.10242779928638047\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.15381700395851858\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.18473956268689637\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.1748203780795721\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.10366996087505834\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.16128449737166292\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.1209906790268814\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.12246461861622243\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.11969075730374647\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.15805288707881213\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.11149515444435354\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.1073558992397675\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.15248206903419353\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.15939395268846362\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.09459717125779954\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.1451432394045924\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.10082155043105877\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.09816051653457493\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.12045451840180073\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.13383089512046042\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.12132685341009819\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.12368423402585228\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.11239489745691891\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.13234456200800493\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.11302471197858271\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.15674060701940523\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.1050696565300448\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.12164452196815533\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.13010492013051284\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.1413478735482996\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.17242036778924943\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.15696186035926665\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.13683273819505629\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.12018545499192959\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.15402639829924214\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.14162609355898892\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.173113087259739\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.11641236623251427\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.17689960152719023\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.11179669013890896\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.1408672661567188\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.14328954488778303\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.16173801707317295\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.1168500040543829\n",
      "Epoch #4 accuracy: train 0.9579833333333333, test 0.9563\n",
      "Epoch #5 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.11085592825706707\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.12922955612403192\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.08154332805706892\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.13373184539753258\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.1208794738305967\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.08797750608173936\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.1355414673656731\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.11769845573796635\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.12549892027556728\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.13345060384802118\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.15661995599802297\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.1365129293329993\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.1245854232867364\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.15801688080830997\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.10387153988475996\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.10908569786644663\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.09895097770589831\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.14012684412678494\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.13764382575443726\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.18791251581437668\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.10743986595834812\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.12932525764336883\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.11896683533434174\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.133453803581388\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.11330193880201875\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.1251478209447039\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.11689610530843966\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.09922317714011851\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.11962437355916111\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.08479141128884439\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.13482537423347668\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.13351514097694667\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.1024276636697119\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.11311321505817583\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.11079984502226918\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.09698193148812204\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.08937440775013976\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.1066848495257388\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.13758898378940274\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.0740571921011017\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.13174468081287607\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.15131222230296493\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.09067945462364868\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.1481688532099498\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.09203002550882976\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.15011926625987873\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.10860619036184703\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.11595846413322246\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.1305158532936586\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.1155972854013649\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.08806428393993801\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.14266634719059318\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.14774030524399742\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.12845854115336477\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.1020266547033811\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.12599074877328195\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.16672483150524212\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.12402568995243003\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.14380262085766862\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.09884360056747037\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.12164825883710742\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.12190692650321122\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.09561732470131841\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.14027063296110004\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.1265684917983792\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.11179059119857321\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.0937452167541319\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.1507419849990731\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.1027120077666436\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.11890457049391417\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.14957593712947634\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.12036973128502541\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.0966540582081818\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.10778670457494219\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.13688061712694508\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.15733245026749176\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.11949787715709567\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.093168802358711\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.13493598944398777\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.08858300742594875\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.1254174415048531\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.1580789626571578\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.1288693214229662\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.12608492182590147\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.09299595181138633\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.10243799355519893\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.10612414861332871\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.13083234686725403\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.12232701912822196\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.17284865391107387\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.1532922626753746\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.10653608599306309\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.12954474361395138\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.10828558903730812\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.12432234243732472\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.11390889069790353\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.13443378133192574\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.13518469012367224\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.1337727033035688\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.09685691829354703\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.10527622972105115\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.0927968178116731\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.08648759794369612\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.08815979170757057\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.14647320119599555\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.14818978844469075\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.14614432178770648\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.08782327213172286\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.13261155312768258\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.12894807011474274\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.12197688225072999\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.10369155834455057\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.11273719152267228\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.1309011281441436\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.08976754935621699\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.0944828611091441\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.09225582791959763\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.11082544860730435\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.10899281277113025\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.13124640521783001\n",
      "Epoch #5 accuracy: train 0.96225, test 0.9595\n",
      "Epoch #6 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.12035457214584641\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.10628065869646365\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.09984227775293053\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.1175519715143704\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.10442517897292718\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.10971249049854279\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.12946542453669366\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.09717488002419336\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.08994951404416532\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.10213875255361188\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.12233711596799168\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.0886608022586663\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.13125281695475915\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.10165672797902614\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.07323813253004254\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.12517335091458365\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.10372220348144613\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.10113145876399651\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.09822409622658677\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.0973087979252009\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.10447528434036281\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.12508765353433898\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.11225314517346917\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.09972047421565773\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.07095396457401247\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.09381123280340477\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.10492357037188947\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.09873510669509132\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.11770905381382948\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.10558723958952632\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.07420744304813438\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.09930202812263846\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.11247572716749962\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.10065368246189646\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.09479523810868024\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.10660805368698817\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.08424424660405759\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.12425757359331663\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.10877272075945577\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.15277969031289165\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.11066984224012244\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.10711678251907962\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.09757579182376705\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.12511700287578356\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.11454702816567114\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.10360208768946114\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.0966520900295084\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.14562323216247017\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.09481280655893796\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.12650040060124595\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.09929278768898044\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.1569299304611349\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.11411557077672277\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.14844028970675185\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.1065085397189275\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.1071929591617196\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.12652603722145575\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.12317589883336379\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.10296602519485776\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.09423171130828127\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.10301867728588729\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.11928581909515634\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.1074840298320707\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.09416184698340473\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.10090406185821035\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.0830419713846847\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.08829698649749612\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.08935479865248315\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.07769828710387992\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.11459748265247706\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.09848745307089402\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.09204945179905408\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.11217750925367917\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.0769367762692706\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.0917595790222298\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.09631124988854546\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.1153251662922413\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.09866307344335178\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.10320687199079175\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.14217118847590968\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.08725909699573807\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.0905951030924505\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.0952008773237986\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.11046974447618921\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.11538766372786734\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.08576693782030628\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.11325603271151506\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.07821619809186378\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.06698451673397161\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.11206135807004525\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.09797234984934095\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.08058519369514001\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.09871151406524928\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.11171548130223397\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.13585654212128254\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.08659241543269938\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.10412501228902059\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.10958551110566887\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.09345502611841028\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.12222975314943944\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.11335920642745771\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.09575640407969044\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.09704429168558121\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.10078826561233085\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.12883742600695536\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.0978390360331521\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.10085727172260092\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.08556671385348819\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.10124351382778429\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.12285679024017343\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.1250092271323132\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.13769219583315648\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.10899128243841655\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.10087693566493783\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.11783002038000029\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.07830826764409533\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.11642767622499084\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.09000007070577183\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.08790018684113618\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.11339352877664412\n",
      "Epoch #6 accuracy: train 0.9670666666666666, test 0.9637\n",
      "Epoch #7 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.11455607478368442\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.06370765077573813\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.10005160036656434\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.11529445261874545\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.09953641247106577\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.10187550770360249\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.07404537775776963\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.09275231985672934\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.0939628487344188\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.07508552878118772\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.08944647013603585\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.0728636029162438\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.09980264208528278\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.08521238005656322\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.07616547232282063\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.08639654896035688\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.08412683437804927\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.10515592175670599\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.10911214338776543\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.10796145999880363\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.10016480658675227\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.10928812491298401\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.09933677061526205\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.12952663819026197\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.09647302810983592\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.0911272010467414\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.07360444387923253\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.11349925824582058\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.10567140449905044\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.10435214137426368\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.0649192275947975\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.09579345147189383\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.08361299038328479\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.09464091080728322\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.08636945288146317\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.07552054093789169\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.09067297265671094\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.10990468810982963\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.12761848600310835\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.07232695233618754\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.12683930367461377\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.0808071706270227\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.07360642271036115\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.08242918779170076\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.12185739933505796\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.12721488801285763\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.10364837723077497\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.10081505577416365\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.08799978762229077\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.09428800130649481\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.07094928854338031\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.08399371827577139\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.0974175448582292\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.09289551742740969\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.09439299075432808\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.11730019357960439\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.0925299085290756\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.121296001026468\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.1362577250399063\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.10573380456416592\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.10998153405358807\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.08996937647332137\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.12036575447092637\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.10026027393990111\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.07085668704256778\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.10578444309435099\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.09547344365343288\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.09765075355137202\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.10156718252673307\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.08077715226391309\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.08384846525548567\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.07861541766599575\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.1003272254134362\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.10886424671936454\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.10828986005513185\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.09207731259558483\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.08836027783603424\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.08619220715607691\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.07918669219741267\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.06864879074565695\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.13949477748898342\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.09398822452755051\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.11357488507416753\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.12635987363789652\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.13526769832165542\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.0821225080444209\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.08158585430577159\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.11975856162673754\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.07996375585468735\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.09488305007934826\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.10642526449185319\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.10230643990812703\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.10909615931044578\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.07314680622804567\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.06520025021766171\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.10660463718475927\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.08250613368280946\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.06753681188782308\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.06618292580010589\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.04457736104987188\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.07309692697022065\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.11760234390106734\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.06664346325162822\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.07384394284207708\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.10094874687994294\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.07631400687509925\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.08465447360886282\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.07463581813669663\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.08718762835578471\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.06230500624753467\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.0704995070949243\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.0789012786231869\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.08501972982221732\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.0888401160344234\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.08692341339015507\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.12222615422075926\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.07058807855372393\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.13185606140753248\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.057091950214198904\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.10497958374449165\n",
      "Epoch #7 accuracy: train 0.9708333333333333, test 0.9669\n",
      "Epoch #8 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.045081605882686206\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.11337361395566475\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.09741732862736224\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.08838714391455471\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.09251804021354554\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.06908137921139763\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.056272574470503156\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.11946290984510805\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.07844798363743342\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.06430498049865985\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.09319554908414568\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.07826153745604436\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.06787814277753851\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.0994039083034865\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.09064938484967995\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.08721382779674701\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.07635100237874902\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.08381646197864202\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.12440650711515785\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.07814074106960894\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.10741754606359083\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.09888956764106743\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.04466284330566238\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.0950722126651518\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.0642056403142334\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.08649349658141867\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.1161489666137304\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.08487071535626396\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.06449140207789876\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.07482736606143078\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.06997974611539945\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.10307185276511675\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.10851287654163301\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.08265968647904574\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.1114923287450051\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.09293303416973077\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.09109603439248193\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.07966527371676284\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.0977303470347936\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.08382144914609643\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.08217998200114465\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.07973793968801375\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.07386648959409711\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.0863543720784398\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.07437211775505718\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.0654472985131157\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.056027566037466794\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.08291695018830741\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.07333568852557912\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.11967630559420653\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.09335620799294003\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.07653464000808739\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.06271463565570021\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.09194064175891543\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.08751025954348368\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.05717468598123399\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.05595777409555349\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.0833640305579583\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.09828962458829006\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.07525785952028721\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.08338207010687486\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.07012721275507044\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.08875404685288234\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.09022161786282083\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.11952762383602891\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.12983754647940104\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.090199978260192\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.06873336289044066\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.09442272107110959\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.06544933602551463\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.12535198973252\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.10447111323504353\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.09584384660025734\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.10049811980029931\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.07299834094578003\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.05389617614128746\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.07108019257463258\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.06305811060647729\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.06373816160415977\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.08036721270217921\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.10046693721547376\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.10879644677004069\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.09725805848408625\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.08376715378763913\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.07470059637808875\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.10964619740303715\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.10160384727595682\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.10155310292180272\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.07065108554088288\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.06893829093825352\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.09360992138812416\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.09133417364851565\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.07110006744812535\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.08386065367905919\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.08707776324692748\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.08470564727947787\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.10793375840946602\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.09532719962684631\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.07114547160299506\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.06763744248684388\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.1038330410259165\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.08078534519540029\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.08588335064649164\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.07343206639577386\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.08051686115022054\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.1124281056689464\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.07290281698455905\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.08550728538876771\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.11200642839351514\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.05474156105342409\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.06112420051574233\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.09294975621348824\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.08525382804790117\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.08563922797045623\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.07375615595529837\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.05870588814729359\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.07936102208272636\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.07472997052032446\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.07197402530390093\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.09415500178654798\n",
      "Epoch #8 accuracy: train 0.9737666666666667, test 0.9689\n",
      "Epoch #9 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.07847690209201036\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.06388937430367309\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.10491362343432376\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.09531166289110189\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.07176067485927873\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.08667017119583158\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.0734480250195895\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.06262750774854876\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.10224287100509165\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.06473615948804638\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.08702338581080502\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.08155407588278615\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.08490831373895741\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.0879978144338022\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.04909175920191396\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.06211099304495581\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.06009021420577727\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.08494182428449394\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.0938013055136506\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.08619481867355508\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.07359358077261027\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.06244868024508813\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.07540726015671141\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.07520580258363649\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.08781171006831472\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.09359710244696623\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.10521280422416664\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.055220221501925966\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.06307542952922107\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.06936066266161998\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.08973500488569647\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.08238429353901518\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.06395712200105516\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.0769487100389103\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.07443587460680476\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.13245918645844892\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.09138655971351867\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.052019173115311616\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.06667812105002685\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.09419312648889261\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.06216544778800259\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.09381755846912065\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.11423520756492286\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.08536224776372607\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.08350009670910544\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.05596390163550025\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.08711530824655306\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.09114017221003191\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.09368474552707125\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.05313170496065572\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.06264362454794593\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.0571572646578842\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.09441525828379838\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.06258835257409516\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.07507568804638559\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.07816524024649237\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.0532882406450052\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.08328111162616796\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.05476457742358289\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.08324226546882572\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.06239133170806708\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.07680023564669082\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.07574650795240928\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.06257456222592994\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.0690446194095876\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.04692912461623978\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.07901983341523328\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.07062092215284689\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.07008980003167332\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.05163632556480431\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.0743010092529197\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.09384594650709073\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.09824580236102806\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.08191399532628911\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.08305310498153946\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.10270646668724626\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.04933975804482792\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.09758975992585178\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.044111423820363156\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.09047421221652974\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.04245437685687605\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.08866219849144756\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.09281014213318675\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.06610941097960087\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.08445521306589963\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.07823147968607844\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.09197564541590311\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.06003753561250041\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.07939146154411818\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.12322188378952144\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.0794166709220093\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.08566199713569465\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.07282615976021603\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.10036709618840231\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.09155772685869855\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.07849473076829624\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.055609096702339385\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.08071154205719215\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.09216443389018034\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.08726226388173075\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.08917210713527626\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.07812863306739244\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.07265948008853855\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.06919253166682424\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.08284094910183469\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.0816729975487212\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.07380205221718604\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.06938316928492248\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.0842827280574574\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.06674976983794917\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.09446287622622193\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.06953016679476574\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.060052361815091576\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.07993260761489732\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.04968271314925647\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.06613851333562994\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.04757457398263364\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.08103983296333259\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.06356149771327123\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.05905972209667066\n",
      "Epoch #9 accuracy: train 0.9766166666666667, test 0.9703\n",
      "Epoch #10 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.07414573323435868\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.08327115352490742\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.07473275135261666\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.06603360285915956\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.05835961253164255\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.04932290318163476\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.09065347620449517\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.06284926735883412\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.10108273212882463\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.059426846605897046\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.07587824021089708\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.06175468194544636\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.07173486953684795\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.060552012844829764\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.071219675728558\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.07532467367365246\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.0889475126795787\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.0690424786635065\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.051225235842707426\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.05580932790523532\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.05766522368372075\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.055235413475274546\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.07547544542303256\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.10831901330975581\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.05536562656805381\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.05668840148662231\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.06479149768551085\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.07488539313776055\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.1003951890964519\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.0573531463844922\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.06392345081364509\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.07564119774710112\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.06853726730331927\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.08831133068903477\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.04870717851824809\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.07741579921704808\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.07962416474580618\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.06774310463681767\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.06285591922673842\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.08115300319763477\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.05621042379157235\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.07054327117874346\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.05977788542878708\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.037687240489715515\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.07056641462843956\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.05298322824396719\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.049461398317122686\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.07705971971568758\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.07318445136136312\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.08605005886642979\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.06018684220252905\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.08078748615892695\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.07933878147258833\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.06753834496349637\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.07577209897341572\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.0675919028556818\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.07526262474536176\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.07834434599048978\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.08520622318829903\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.08598116465967866\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.057753206532461\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.07089843211757933\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.058399306191984066\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.08774820131139544\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.04773601242877309\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.0457272749167158\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.08066550100009595\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.1274763577155736\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.08615578428562036\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.08219587809836323\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.06913841090301918\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.06951821068089088\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.05070891390934624\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.06780050023821647\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.08684757375664623\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.054830166535294766\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.07056030092661446\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.09106185647348874\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.07621995536419504\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.06508096229474057\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.07644215369918567\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.07586266316220502\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.030768367792079997\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.07557436231633287\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.045960391934652706\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.08016335246522145\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.09172877568377266\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.07577059745403972\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.0947877734411306\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.06351444763199869\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.07208420020572019\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.04954543581648912\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.07137557253582842\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.06333121504305879\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.06320958767186353\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.06032732167809307\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.06131966405195322\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.04632516337322722\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.0722406104053733\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.07906564139181163\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.08378118640498239\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.07049687247083368\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.060755175650665835\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.08109624088273795\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.09275477461508316\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.07371748083876134\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.09812359314236056\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.04706050635650198\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.04925175648458391\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.05766096184451807\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.11204035424578311\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.06591542379624661\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.07107348564214448\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.05829151096513968\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.07874961301782656\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.0630863348750986\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.08798879620859328\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.052994523835276426\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.07627322849454683\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.058810979606928666\n",
      "Epoch #10 accuracy: train 0.97805, test 0.9712\n",
      "Epoch #11 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.04964603972675094\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.06968084099009045\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.0637806053349456\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.07203508044180316\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.07104822919611413\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.07658043184665894\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.054225638400273196\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.05859206999561842\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.0690694185635494\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.049621631678753296\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.053651798189407046\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.07931316274641273\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.05359754048374874\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.06625157232485004\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.05398832052977889\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.055675406168052506\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.050526399325141295\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.07021345368462746\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.07184227685235035\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.0566474054614836\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.041664706638212486\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.07958919303562022\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.10419448210032271\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.060657554200887046\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.06382259794093516\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.10007228089737888\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.06483412483284767\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.04829584939444536\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.07552902143328535\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.06059621193901151\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.0692646623934866\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.08392178827108296\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.06292194414226522\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.04315780736815079\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.054430557820138906\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.07210806311298021\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.0765869304599498\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.0714103096148122\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.06327457324500226\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.05195655649861288\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.0708515640289635\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.0762954461711418\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.058209378385371\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.07853561883567854\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.05918010782718456\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.07458366405849048\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.046430443428041826\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.061392353494811855\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.08041930167495465\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.07938323874756115\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.05482204093202616\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.05504041035441523\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.07177294578987045\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.06099297659318064\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.056022590108306944\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.0778400811770602\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.05629768446061696\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.059298793157100675\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.06583263283013148\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.07477352655049083\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.06510880829888631\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.06107414090604881\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.07532101523157632\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.06885129067818553\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.060257977717036\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.06127250264456464\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.049369755194019885\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.07586287604695292\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.05456142966650023\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.05655971700628382\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.06028551084788535\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.0874139781886753\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.06546987512842738\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.04303156880449705\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.05855529508385196\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.05924675080403769\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.0672237545640237\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.041825712132935906\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.05995716877687082\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.04729632469586237\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.10432845279767462\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.0558039597058973\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.08251518211210092\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.07461383128586971\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.03393993756117994\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.05547206759172097\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.07139753584147522\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.0555394168854939\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.061744392588264536\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.07749557263467667\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.05676002071626105\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.06129811341812973\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.053509766735773436\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.06881769485804035\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.05413530970880749\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.053016482414691174\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.035301183266649536\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.05281968371154339\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.05063768083631596\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.06578928488411452\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.04524362659347891\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.08022453231791832\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.0688654136989536\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.07940588211212676\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.05880514008829067\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.05740236447165535\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.06863001449959927\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.07528488602409023\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.08589581816227407\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.0559783825247191\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.08800121954953814\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.07353010926061662\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.06415656311885264\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.056749380059965236\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.06786131127389056\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.07196170653249245\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.05561145093510522\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.05187976735276744\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.11185246881030267\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.06997957936778673\n",
      "Epoch #11 accuracy: train 0.9802166666666666, test 0.9735\n",
      "Epoch #12 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.05467092031680073\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.052109868808667206\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.04330616226405365\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.0757392551831556\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.0568569285563801\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.06344026966148202\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.046465965760487865\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.05998481638226312\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.06428946645651258\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.05563818113378515\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.055708557205987295\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.07760603211688874\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.06764290713708834\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.07242096952210955\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.06116510608402809\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.0724814460349456\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.06623842686448003\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.0633273320327154\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.06958934626399937\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.05097043271104598\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.07219167419809643\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.05138843880994384\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.05008822138593326\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.058408446525125404\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.10062450189871075\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.06996565231756907\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.05135012542618484\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.10309478025921424\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.0525222995670877\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.05074250233698358\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.05349201675042101\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.0704076293574765\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.0634640319344153\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.042847218087861495\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.058792327297648043\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.047818021770130274\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.036350784241770046\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.06798804621933725\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.04946960056238585\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.07434466399522777\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.09351709714877333\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.055786147181031254\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.07977811516153537\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.05566983597879555\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.05930751377654539\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.05677699407454817\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.057220379824010736\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.04506712200572176\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.049895713074792285\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.05301424394691861\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.0541891397289021\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.051939721853957434\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.05284333988574063\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.05408389921511506\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.0489309775127781\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.06206300209733065\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.07950951941910751\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.05689317592947765\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.06925728091667409\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.0582542519888164\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.055000363333802094\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.040142115753542776\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.062310137932800845\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.07654604406611357\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.04308581305435564\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.06693486140047214\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.06661253073510238\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.03790700758810656\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.05157277130830745\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.06154908418020002\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.06148310626628391\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.07982614499846552\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.055347451963776176\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.04577456074024279\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.06375476878195939\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.06349708733533212\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.09878975145199885\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.04669196386908801\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.03376109053697994\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.04767312893118254\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.03711051492562437\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.0443415386844758\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.07717903784829028\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.05308257006960167\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.0555452044309476\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.058785570094005236\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.06600169803951014\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.05986879382107456\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.044278480852538014\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.049451353885917045\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.05196722645931724\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.06234392256728559\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.057517838984562586\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.05394170155971808\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.04042427352835567\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.07675400024741985\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.0687570323101187\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.07194527872377762\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.08937374051402801\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.04031547768670029\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.04972461496387823\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.062329558183109084\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.06386657288840138\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.061907579877172686\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.05518673893877002\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.061435237365716884\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.05184958793997364\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.0976447962937781\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.047087215483765696\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.06331445388336213\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.05721727674287885\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.07802982928186483\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.05005708993234948\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.06589470240562004\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.055077620126673506\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.031825692785907245\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.04930967604685708\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.04997362584147069\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.07002531632818738\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.03888230190583917\n",
      "Epoch #12 accuracy: train 0.9796666666666667, test 0.9717\n",
      "Epoch #13 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.05863302681231981\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.04488443614573162\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.04904637257749008\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.052239220469899136\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.047975197814795936\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.03834850027179828\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.056615913975606065\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.06849045330681713\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.07087750135527528\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.06757753190201922\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.05719099758438065\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.034133521295707035\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.03861375445646154\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.09813952293663211\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.07081005569040372\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.06839577107338733\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.03284278420957854\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.05437815577388236\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.045978359648943395\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.04289618350959805\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.0858500496720406\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.0662583859812198\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.05558422893695021\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.048763132929146163\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.031074680907290618\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.04658263394110226\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.09652721463823329\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.05132618658733315\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.06228624759595671\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.05016342538028094\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.05859028594254229\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.04130555623580914\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.04607076384215283\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.054466422543332144\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.030074532041221874\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.06230466087077073\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.05025166790861845\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.05260899728739004\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.0849673121737177\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.05901720918830024\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.05330175553723825\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.08289456155397266\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.0598603160755518\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.05023836556797364\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.048328410010054376\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.04401387428149975\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.05272645924820684\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.05323150255019133\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.041364051386750485\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.048375428120133714\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.0474378207498835\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.09624775083606382\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.03930900348280618\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.08950510540819268\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.05846672724167715\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.042740150775208735\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.033696610761875474\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.064970729121899\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.05115811906767162\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.07222602753441418\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.047787189188370806\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.048047515143852\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.07206695686688123\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.05109627520452761\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.03831729204077223\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.04510956581412715\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.05390372357934549\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.069913211300385\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.0596930644202601\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.06377276398209031\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.05485031202200854\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.06321523548892578\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.04915461946871207\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.05758636076814319\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.0641032310148214\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.05030695142455092\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.06770692483246987\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.04159942802832557\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.04660185305556482\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.049426499666685546\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.0475615783016532\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.05057110954089602\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.06953171328163954\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.07532916359738909\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.057627180825322935\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.04080020399804195\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.06190015497066348\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.05447655849277316\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.06671010774447002\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.05929960602429354\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.06177883603247303\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.04655231171772672\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.08628516096267838\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.0544136710740122\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.04622196403150923\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.06980486256262361\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.056506419411267717\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.0485751364709204\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.045058888351698044\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.05976085892253982\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.0658126954846112\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.049320877013621256\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.056131303768543875\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.042429259276518666\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.030559925502646103\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.058595776753917674\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.04747490809853677\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.057300499760293805\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.03270520187486014\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.04092454680922851\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.045918238859421104\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.04731211301778869\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.0897712847464287\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.044200354727006626\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.0547426219215272\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.04955251913666963\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.05271989052291985\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.04830761428109051\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.05897220294598676\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.041310836770459464\n",
      "Epoch #13 accuracy: train 0.9811166666666666, test 0.9721\n",
      "Epoch #14 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.06672194800316678\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.025208704000248765\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.03842788044434238\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.04071112392071241\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.04585501717847187\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.04472325755648583\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.03457466208632408\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.051096463225323956\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.04545107272361163\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.04530200187743077\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.03733724164040146\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.03493627507428251\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.048556506460904716\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.04149799506634669\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.08378333649723163\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.051582152159912825\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.037660505012500525\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.05003445308208311\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.04665594747997701\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.03145326852908978\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.030756864960632128\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.04488785489306214\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.048774875540369834\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.05026172178938656\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.0730593419271308\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.053453663679232565\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.053723652493276444\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.07234266590807553\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.07059800440896405\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.05434072488006985\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.04447358517260797\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.06534790497368509\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.06923297089215501\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.048158587001326546\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.056506518352584974\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.04318679881462199\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.04732375078903968\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.05624114060554987\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.05775679640568775\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.06164365555071869\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.042168502436925\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.05041346383757319\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.06521092446072319\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.037260145637027564\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.059341342642591\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.05980401544252857\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.07451832481599024\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.049119963995909656\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.05606555706294904\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.04456508734237428\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.07159523097572593\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.07631289254005014\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.036600469471677916\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.05236827373583144\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.04856813097641556\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.04249514475217311\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.08434760269411978\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.042241745770347484\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.062141932608414364\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.03937374078612218\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.06342834055673115\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.03573325028990136\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.040414371267927636\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.05160596284628613\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.08537218755177273\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.05275494329899567\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.04235273222582445\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.025753642282392113\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.029470505082694103\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.035124932678694654\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.04658207055994157\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.055797481064797594\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.05228393078169837\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.05634084374122329\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.05879814058852197\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.06554175161226913\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.052450543767824855\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.04700952116768217\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.058662075858900924\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.06045792586944626\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.06551144188340866\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.04499970233218623\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.055321128701710034\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.05806516139387427\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.03928425950576418\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.04845552970175839\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.04416863739459179\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.0718260120133988\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.05007313830801776\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.03744427712704313\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.053018279388134956\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.042620072658738836\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.0869433941406663\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.027828733648461217\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.06888818106462284\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.050845000498283006\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.06536682791601982\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.047415016969816245\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.04217062580052776\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.043524372267919414\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.04040235221625754\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.041998980364978934\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.06862934455985856\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.05192939051402048\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.07140619703038922\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.05068862396600606\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.04004519830151672\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.04452351511090003\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.057477306488176484\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.03981254432838024\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.03706405003308783\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.04727841562459871\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.049141495831384795\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.05493307550462678\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.03385011487691015\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.04431380097666152\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.07516701869330329\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.03958238780594087\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.06013930876483926\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.044076136119850715\n",
      "Epoch #14 accuracy: train 0.9832833333333333, test 0.9751\n",
      "Epoch #15 ---\n",
      "  Mini-Batch: 0...500\n",
      "  Loss: 0.04604460333710987\n",
      "  Mini-Batch: 500...1000\n",
      "  Loss: 0.05285482451181188\n",
      "  Mini-Batch: 1000...1500\n",
      "  Loss: 0.05385916899387659\n",
      "  Mini-Batch: 1500...2000\n",
      "  Loss: 0.06828226280259976\n",
      "  Mini-Batch: 2000...2500\n",
      "  Loss: 0.05366180960673221\n",
      "  Mini-Batch: 2500...3000\n",
      "  Loss: 0.02737353473462897\n",
      "  Mini-Batch: 3000...3500\n",
      "  Loss: 0.05170881746822208\n",
      "  Mini-Batch: 3500...4000\n",
      "  Loss: 0.04038432558566204\n",
      "  Mini-Batch: 4000...4500\n",
      "  Loss: 0.05648458608850511\n",
      "  Mini-Batch: 4500...5000\n",
      "  Loss: 0.03436548376439429\n",
      "  Mini-Batch: 5000...5500\n",
      "  Loss: 0.04756902300337459\n",
      "  Mini-Batch: 5500...6000\n",
      "  Loss: 0.05306788403713006\n",
      "  Mini-Batch: 6000...6500\n",
      "  Loss: 0.060572531870879714\n",
      "  Mini-Batch: 6500...7000\n",
      "  Loss: 0.045345809515817494\n",
      "  Mini-Batch: 7000...7500\n",
      "  Loss: 0.08929019133352874\n",
      "  Mini-Batch: 7500...8000\n",
      "  Loss: 0.03578121424447681\n",
      "  Mini-Batch: 8000...8500\n",
      "  Loss: 0.059505503655490725\n",
      "  Mini-Batch: 8500...9000\n",
      "  Loss: 0.05471929375291393\n",
      "  Mini-Batch: 9000...9500\n",
      "  Loss: 0.04788409033294946\n",
      "  Mini-Batch: 9500...10000\n",
      "  Loss: 0.03455926147515603\n",
      "  Mini-Batch: 10000...10500\n",
      "  Loss: 0.03276953716486269\n",
      "  Mini-Batch: 10500...11000\n",
      "  Loss: 0.030775940456305297\n",
      "  Mini-Batch: 11000...11500\n",
      "  Loss: 0.07297186788922853\n",
      "  Mini-Batch: 11500...12000\n",
      "  Loss: 0.04760506156420509\n",
      "  Mini-Batch: 12000...12500\n",
      "  Loss: 0.07063913337717873\n",
      "  Mini-Batch: 12500...13000\n",
      "  Loss: 0.03635564598955804\n",
      "  Mini-Batch: 13000...13500\n",
      "  Loss: 0.03376742841692864\n",
      "  Mini-Batch: 13500...14000\n",
      "  Loss: 0.04749815640738061\n",
      "  Mini-Batch: 14000...14500\n",
      "  Loss: 0.03724263225145345\n",
      "  Mini-Batch: 14500...15000\n",
      "  Loss: 0.038660472019221\n",
      "  Mini-Batch: 15000...15500\n",
      "  Loss: 0.048141073104121704\n",
      "  Mini-Batch: 15500...16000\n",
      "  Loss: 0.06971779797866694\n",
      "  Mini-Batch: 16000...16500\n",
      "  Loss: 0.03466581407882751\n",
      "  Mini-Batch: 16500...17000\n",
      "  Loss: 0.046732325555966125\n",
      "  Mini-Batch: 17000...17500\n",
      "  Loss: 0.04052291170874122\n",
      "  Mini-Batch: 17500...18000\n",
      "  Loss: 0.057345455200926265\n",
      "  Mini-Batch: 18000...18500\n",
      "  Loss: 0.08193425853023323\n",
      "  Mini-Batch: 18500...19000\n",
      "  Loss: 0.03891412826253951\n",
      "  Mini-Batch: 19000...19500\n",
      "  Loss: 0.07241553952555649\n",
      "  Mini-Batch: 19500...20000\n",
      "  Loss: 0.033119178787837315\n",
      "  Mini-Batch: 20000...20500\n",
      "  Loss: 0.03800103300753902\n",
      "  Mini-Batch: 20500...21000\n",
      "  Loss: 0.03839983473939115\n",
      "  Mini-Batch: 21000...21500\n",
      "  Loss: 0.05468438496286286\n",
      "  Mini-Batch: 21500...22000\n",
      "  Loss: 0.05518144269960115\n",
      "  Mini-Batch: 22000...22500\n",
      "  Loss: 0.054170711933298706\n",
      "  Mini-Batch: 22500...23000\n",
      "  Loss: 0.044644214972118124\n",
      "  Mini-Batch: 23000...23500\n",
      "  Loss: 0.04671690026475784\n",
      "  Mini-Batch: 23500...24000\n",
      "  Loss: 0.05476423436228025\n",
      "  Mini-Batch: 24000...24500\n",
      "  Loss: 0.05491644568330639\n",
      "  Mini-Batch: 24500...25000\n",
      "  Loss: 0.037554788230686274\n",
      "  Mini-Batch: 25000...25500\n",
      "  Loss: 0.04868986247756195\n",
      "  Mini-Batch: 25500...26000\n",
      "  Loss: 0.0495444381675369\n",
      "  Mini-Batch: 26000...26500\n",
      "  Loss: 0.048800787777407185\n",
      "  Mini-Batch: 26500...27000\n",
      "  Loss: 0.04337140021163601\n",
      "  Mini-Batch: 27000...27500\n",
      "  Loss: 0.03821469834765975\n",
      "  Mini-Batch: 27500...28000\n",
      "  Loss: 0.044345898023374714\n",
      "  Mini-Batch: 28000...28500\n",
      "  Loss: 0.048268179407742265\n",
      "  Mini-Batch: 28500...29000\n",
      "  Loss: 0.038099231754764915\n",
      "  Mini-Batch: 29000...29500\n",
      "  Loss: 0.037803066742742836\n",
      "  Mini-Batch: 29500...30000\n",
      "  Loss: 0.043790655206106484\n",
      "  Mini-Batch: 30000...30500\n",
      "  Loss: 0.05724453233322067\n",
      "  Mini-Batch: 30500...31000\n",
      "  Loss: 0.07187450067103\n",
      "  Mini-Batch: 31000...31500\n",
      "  Loss: 0.043519227099263286\n",
      "  Mini-Batch: 31500...32000\n",
      "  Loss: 0.04660903297853321\n",
      "  Mini-Batch: 32000...32500\n",
      "  Loss: 0.047364010759897664\n",
      "  Mini-Batch: 32500...33000\n",
      "  Loss: 0.04101261893943476\n",
      "  Mini-Batch: 33000...33500\n",
      "  Loss: 0.043767435077026406\n",
      "  Mini-Batch: 33500...34000\n",
      "  Loss: 0.054367131255584386\n",
      "  Mini-Batch: 34000...34500\n",
      "  Loss: 0.05025542616238559\n",
      "  Mini-Batch: 34500...35000\n",
      "  Loss: 0.04034016722227676\n",
      "  Mini-Batch: 35000...35500\n",
      "  Loss: 0.053660052011943744\n",
      "  Mini-Batch: 35500...36000\n",
      "  Loss: 0.04639512404600606\n",
      "  Mini-Batch: 36000...36500\n",
      "  Loss: 0.06496063563324363\n",
      "  Mini-Batch: 36500...37000\n",
      "  Loss: 0.06420923946163438\n",
      "  Mini-Batch: 37000...37500\n",
      "  Loss: 0.054623397535861505\n",
      "  Mini-Batch: 37500...38000\n",
      "  Loss: 0.05900572980824495\n",
      "  Mini-Batch: 38000...38500\n",
      "  Loss: 0.03718984835141673\n",
      "  Mini-Batch: 38500...39000\n",
      "  Loss: 0.028129788769159437\n",
      "  Mini-Batch: 39000...39500\n",
      "  Loss: 0.049493278343029586\n",
      "  Mini-Batch: 39500...40000\n",
      "  Loss: 0.047944720378615344\n",
      "  Mini-Batch: 40000...40500\n",
      "  Loss: 0.036066890033926505\n",
      "  Mini-Batch: 40500...41000\n",
      "  Loss: 0.04883109089889224\n",
      "  Mini-Batch: 41000...41500\n",
      "  Loss: 0.054940926670200174\n",
      "  Mini-Batch: 41500...42000\n",
      "  Loss: 0.06096177115861372\n",
      "  Mini-Batch: 42000...42500\n",
      "  Loss: 0.04463336259480678\n",
      "  Mini-Batch: 42500...43000\n",
      "  Loss: 0.04324924373774979\n",
      "  Mini-Batch: 43000...43500\n",
      "  Loss: 0.03343494583795133\n",
      "  Mini-Batch: 43500...44000\n",
      "  Loss: 0.052511719525972514\n",
      "  Mini-Batch: 44000...44500\n",
      "  Loss: 0.04612534543353117\n",
      "  Mini-Batch: 44500...45000\n",
      "  Loss: 0.04432335721118182\n",
      "  Mini-Batch: 45000...45500\n",
      "  Loss: 0.03289810261292571\n",
      "  Mini-Batch: 45500...46000\n",
      "  Loss: 0.044500224948882734\n",
      "  Mini-Batch: 46000...46500\n",
      "  Loss: 0.038554731509467574\n",
      "  Mini-Batch: 46500...47000\n",
      "  Loss: 0.06686337191141684\n",
      "  Mini-Batch: 47000...47500\n",
      "  Loss: 0.07544550715525242\n",
      "  Mini-Batch: 47500...48000\n",
      "  Loss: 0.04624302530622033\n",
      "  Mini-Batch: 48000...48500\n",
      "  Loss: 0.03785014008132823\n",
      "  Mini-Batch: 48500...49000\n",
      "  Loss: 0.05135035337670893\n",
      "  Mini-Batch: 49000...49500\n",
      "  Loss: 0.05126920083965626\n",
      "  Mini-Batch: 49500...50000\n",
      "  Loss: 0.048609723649624285\n",
      "  Mini-Batch: 50000...50500\n",
      "  Loss: 0.03670453678237112\n",
      "  Mini-Batch: 50500...51000\n",
      "  Loss: 0.05714691255062763\n",
      "  Mini-Batch: 51000...51500\n",
      "  Loss: 0.05032064253947493\n",
      "  Mini-Batch: 51500...52000\n",
      "  Loss: 0.030829244579577697\n",
      "  Mini-Batch: 52000...52500\n",
      "  Loss: 0.024597925173808575\n",
      "  Mini-Batch: 52500...53000\n",
      "  Loss: 0.035399138445741815\n",
      "  Mini-Batch: 53000...53500\n",
      "  Loss: 0.03766293725433587\n",
      "  Mini-Batch: 53500...54000\n",
      "  Loss: 0.06166836131208218\n",
      "  Mini-Batch: 54000...54500\n",
      "  Loss: 0.06118034255575388\n",
      "  Mini-Batch: 54500...55000\n",
      "  Loss: 0.0545034149472904\n",
      "  Mini-Batch: 55000...55500\n",
      "  Loss: 0.05393093827659894\n",
      "  Mini-Batch: 55500...56000\n",
      "  Loss: 0.05358326717349161\n",
      "  Mini-Batch: 56000...56500\n",
      "  Loss: 0.04904740708122223\n",
      "  Mini-Batch: 56500...57000\n",
      "  Loss: 0.045008545540705824\n",
      "  Mini-Batch: 57000...57500\n",
      "  Loss: 0.03956070057796768\n",
      "  Mini-Batch: 57500...58000\n",
      "  Loss: 0.051269277511355134\n",
      "  Mini-Batch: 58000...58500\n",
      "  Loss: 0.03922066215881844\n",
      "  Mini-Batch: 58500...59000\n",
      "  Loss: 0.04083415374980591\n",
      "  Mini-Batch: 59000...59500\n",
      "  Loss: 0.03882566277038011\n",
      "  Mini-Batch: 59500...60000\n",
      "  Loss: 0.02721163250566821\n",
      "Epoch #15 accuracy: train 0.9844333333333334, test 0.9741\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# 네트워크 준비 ---\n",
    "net = MyAwesomeModelV2(\n",
    "  input_size  = 28 * 28, # MNIST의 사진 크기가 28 픽셀 곱하기 28이므로 입력층 28*28노드\n",
    "  hidden_size = 100,     # 은닉층 퍼셉트론 100개\n",
    "  output_size = 10       # 0부터 10까지를 예측하는 classification 문제이므로 출력층 퍼셉트론 10개 \n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "# 데이터셋 준비 --- \n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "  mnist_loader.load_dataset(normalize=True, one_hot_label=True)\n",
    "\n",
    "\n",
    "#\n",
    "# 하이퍼파라미터 ---\n",
    "\n",
    "epochs        = 16                 # 데이터셋 전체 학습 횟수\n",
    "train_size    = x_train.shape[0]   # 총 데이터셋의 개수\n",
    "batch_size    = 500                # 미니배치 크기\n",
    "learning_rate = 0.5                # 경사하강법 lr\n",
    "\n",
    "\n",
    "#\n",
    "# 학습 시작\n",
    "\n",
    "# 한 에폭 시작\n",
    "for epoch_idx in range(epochs):\n",
    "  print(f\"Epoch #{epoch_idx} ---\")\n",
    "\n",
    "  # 데이터셋 섞기 (데이터셋이 혹시나 제대로 분포되어 있지 않을 수 있어 한번 섞어준다. ex: 앞에는 정답이 0인 사진만 있는다던지...)\n",
    "  s = np.arange(0, train_size, 1)\n",
    "  np.random.shuffle(s)\n",
    "\n",
    "  x_train = x_train[s]\n",
    "  t_train = t_train[s]\n",
    "\n",
    "  # 한 미니배치 생성\n",
    "  for iter_start in range(0, train_size, batch_size):\n",
    "    # print(f\"  Mini-Batch: {iter_start}...{iter_start+batch_size}\")\n",
    "\n",
    "    x_batch = x_train[iter_start:iter_start+batch_size]\n",
    "    t_batch = t_train[iter_start:iter_start+batch_size]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = net.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 기울기 반영\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "      net.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 손실율 기록\n",
    "    loss = net.loss(x_batch, t_batch)\n",
    "    # print(f\"  Loss: {loss}\")\n",
    "\n",
    "  # 에폭 종료후 정확도 계산\n",
    "  train_acc = net.accuracy(x_train, t_train)\n",
    "  test_acc = net.accuracy(x_test, t_test)\n",
    "\n",
    "  print(f\"Epoch #{epoch_idx} accuracy: train {train_acc}, test {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상당히 빨라진 것을 볼 수 있다!\\\n",
    "최종 정확도가 학습 데이터는 99.95%, 테스트 데이터는 98.23%로 정확도가 높은 모델이 완성되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: 1\n",
      "Answer: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.randint(0, x_test.shape[0])\n",
    "x = x_test[n:n+1]\n",
    "t = t_test[n]\n",
    "\n",
    "y = net.predict(x)[0]\n",
    "\n",
    "print(f\"Predict: {np.argmax(y)}\")\n",
    "print(f\"Answer: {np.argmax(t)}\")\n",
    "\n",
    "plt.imshow(np.reshape(x[0], (28, 28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 알고리즘들 (Optimizers)\n",
    "이제는 경사하강법 말고도 다른 최적화 방법도 알아보자. \\\n",
    "참고로 loss 함수가 최저가 되도록 하는 w와 b를 구하는 알고리즘을 최적화 알고리즘이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimizer\n",
    "이때까지 봤던 확률적 경사 하강법은 앞 뒤로 왔다갔다 하는 상황이 많이 발생한다.\n",
    "\n",
    "그래서 Momentum 알고리즘은 경사진 곳에서 공을 굴렸을때 하강하는 것처럼 물리법칙을 활용해 하강하는 것을 말한다.\\\n",
    "공이 바닥을 구르게 되므로 경사 하강법 보다 더 안정적인 모습을 보이게 된다.\n",
    "\n",
    "<img src=\"img/epW89.jpg\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "경사하강법의 두번째 문제는 초기에 설정한 Learning Rate를 계속 사용하는데 있다\\\n",
    "하강을 하면 할수록 Learning Rate를 줄여가며 하강하면 초반에는 하강을 빠르게 하고\\\n",
    "하강이 어느정도 된 상태에서는 더 미세하게 이동할 수 있지 않을까?\n",
    "\n",
    "그래서 AdaGrad는 각각의 개발 매개변수에 학습률을 점차적으로 조정하며 진행한다.\n",
    "\n",
    "<img src=\"img/output_adagrad_2fb0ed_6_1.svg\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "Adam은 이 두 방법을 둘다 사용하는 알고리즘으로 매개변수의 갱신 정도를 능동적으로 조정하면서\\\n",
    "모멘텀 물리 법칙을 따라 튀는 현상을 억제한다.\n",
    "\n",
    "<img src=\"img/public.avif\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 초기값 설정\n",
    "뉴럴 네트워크에서 학습전 가중치의 초기값을 적절히 설정하는 것은 매우 중요하다.\n",
    "\n",
    "만약 가중치의 초기값이 적절히 분포되어 있지 않는 경우\\\n",
    "미분시 기울기가 0이되어 모델 학습이 개선되지 않는 기울기 소실 문제와\\\n",
    "한 레이어의 많은 퍼셉트론의 결과값이 서로 비슷하게 되어 퍼셉트론을 여러개 두는 의미가 없어지는 표현력 문제가 있다.\n",
    "\n",
    "이렇기 때문에 딥러닝 학계에서는 여러가지 방법을 시도해 보고 있다.\n",
    "\n",
    "다음과 같은 간단한 모델이 있다고 생각하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_w_init(f, h=sigmoid):\n",
    "  input_data = np.random.randn(1000, 100)\n",
    "  node_num = 100\n",
    "  hidden_layer_size = 5\n",
    "  activations = {}\n",
    "\n",
    "  x = input_data\n",
    "\n",
    "  for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "      x = activations[i-1]\n",
    "\n",
    "    w = f(node_num)\n",
    "    a = np.dot(x, w)\n",
    "    z = h(a)\n",
    "\n",
    "    activations[i] = z\n",
    "    \n",
    "  for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규분포를 활용한 초깃값\n",
    "표준편차를 가진 임의의 w값들을 설정해두면 여러 레이어 이후 어떻게 될까?\n",
    "\n",
    "흠... 각 w값들이 0과 1에 치우쳐저 있어 층이 깊어져도 크게 변하지 않는 것을 볼 수 있다.\\\n",
    "이렇게 파라미터가 0과 1 한쪽에 치우쳐저 있는 경우 기울기가 소실되어 수정이 일어나지 않는것을 기울기 소실이라 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(node_num):\n",
    "  return np.random.randn(node_num, node_num)\n",
    "\n",
    "test_w_init(f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준편차를 0.01로 조절하면 어떻게 될까?\n",
    "\n",
    "이번엔 0.5에 치우쳐저 있게 된다. 이러면 기울기가 소실되지는 않지만 모든 퍼셉트론이 비슷한 출력을 할것이므로\\\n",
    "퍼셉트론을 하나 하는것과 별반 다른게 없어지게 된다. 이것을 \"포현력을 제한한다\"라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(node_num):\n",
    "  return np.random.randn(node_num, node_num) * 0.01\n",
    "\n",
    "test_w_init(f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 표준 분포를 0.1로 하면 고르게 분포된다. 하지만 매번 표준 분포를 이런식으로 예측하는 것은 매우 까다롭다.\n",
    "\n",
    "자동으로 분포를 고르게 해주는 알고리즘들을 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(node_num):\n",
    "  return np.random.randn(node_num, node_num) * 0.1\n",
    "\n",
    "test_w_init(f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier 초깃값\n",
    "Xavier는 표준 편차가 `1/sqrt(퍼셉트론_수)`인 정규분포를 활용한 초깃값 설정 알고리즘이다.\\\n",
    "Xavier 알고리즘을 사용하면 가중치의 분포가 넓어지게 되지만 레이어를 지날때마다 분포해둔것이 뭉개지는 현상이 발생한다.\n",
    "\n",
    "코드로 작성하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(node_num):\n",
    "  return np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
    "\n",
    "test_w_init(f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제는 활성화 함수를 sigmoid가 아닌 tanh으로 하였을때 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tanh(x):\n",
    "  return np.tanh(x)\n",
    "\n",
    "def f(node_num):\n",
    "  return np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
    "\n",
    "test_w_init(f, h=tanh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He 초깃값\n",
    "활성화 함수가 ReLU일때에는 He 초깃값 알고리즘을 통해 더 나은 분포를 보여줄 수 있다.\n",
    "\n",
    "He 초깃값은 `sqrt(2/퍼셉트론_수)`의 표준 편차를 보이는 정규 분포로 다음과 같이 확인할 수 있다.\n",
    "\n",
    "He는 다른 알고리즘과 달리 레이어가 깊어져도 고른 분포를 보여주기에 자주 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(node_num):\n",
    "  return np.random.randn(node_num, node_num) * np.sqrt(2/node_num)\n",
    "\n",
    "test_w_init(f, h=relu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 정규화 알고리즘\n",
    "초기값을 복잡하게 설정하지 말고 각 층에 출력값을 강제로 분포하도록 설정하면 어떻게 될까?\n",
    "\n",
    "이런 생각에서 나온 알고리즘이 바로 배치 정규화 레이어이다.\\\n",
    "배치 정규화 레이어는 은닉층 퍼셉트론과 활성화 함수 사이, 혹은 활성화 함수 뒤에 넣을때 효과가 좋은것으로 알려져 있다.\n",
    "\n",
    "배치 정규화 레이어는 다음과 같이 코드로 작성할 수 있으나 설명하기 어려우므로 코드만 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "  \"\"\"\n",
    "  http://arxiv.org/abs/1502.03167\n",
    "  \"\"\"\n",
    "  def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "    self.gamma = gamma\n",
    "    self.beta = beta\n",
    "    self.momentum = momentum\n",
    "    self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
    "\n",
    "    # 시험할 때 사용할 평균과 분산\n",
    "    self.running_mean = running_mean\n",
    "    self.running_var = running_var  \n",
    "    \n",
    "    # backward 시에 사용할 중간 데이터\n",
    "    self.batch_size = None\n",
    "    self.xc = None\n",
    "    self.std = None\n",
    "    self.dgamma = None\n",
    "    self.dbeta = None\n",
    "\n",
    "  def forward(self, x, train_flg=True):\n",
    "    self.input_shape = x.shape\n",
    "    if x.ndim != 2:\n",
    "      N, C, H, W = x.shape\n",
    "      x = x.reshape(N, -1)\n",
    "\n",
    "    out = self.__forward(x, train_flg)\n",
    "      \n",
    "    return out.reshape(*self.input_shape)\n",
    "          \n",
    "  def __forward(self, x, train_flg):\n",
    "    if self.running_mean is None:\n",
    "      N, D = x.shape\n",
    "      self.running_mean = np.zeros(D)\n",
    "      self.running_var = np.zeros(D)\n",
    "                    \n",
    "    if train_flg:\n",
    "      mu = x.mean(axis=0)\n",
    "      xc = x - mu\n",
    "      var = np.mean(xc**2, axis=0)\n",
    "      std = np.sqrt(var + 10e-7)\n",
    "      xn = xc / std\n",
    "      \n",
    "      self.batch_size = x.shape[0]\n",
    "      self.xc = xc\n",
    "      self.xn = xn\n",
    "      self.std = std\n",
    "      self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "      self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "    else:\n",
    "      xc = x - self.running_mean\n",
    "      xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "        \n",
    "    out = self.gamma * xn + self.beta \n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    if dout.ndim != 2:\n",
    "      N, C, H, W = dout.shape\n",
    "      dout = dout.reshape(N, -1)\n",
    "\n",
    "    dx = self.__backward(dout)\n",
    "\n",
    "    dx = dx.reshape(*self.input_shape)\n",
    "    return dx\n",
    "\n",
    "  def __backward(self, dout):\n",
    "    dbeta = dout.sum(axis=0)\n",
    "    dgamma = np.sum(self.xn * dout, axis=0)\n",
    "    dxn = self.gamma * dout\n",
    "    dxc = dxn / self.std\n",
    "    dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "    dvar = 0.5 * dstd / self.std\n",
    "    dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "    dmu = np.sum(dxc, axis=0)\n",
    "    dx = dxc - dmu / self.batch_size\n",
    "    \n",
    "    self.dgamma = dgamma\n",
    "    self.dbeta = dbeta\n",
    "    \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화는 가중치 초기값 설정 알고리즘을 선택할 필요도 없고 학습을 빠르게 진행시킬 수 있어 애용된다.\n",
    "\n",
    "학습을 빠르게 진행시키는 이유는 손실 함수의 그래프를 다음과 같이 Smoothing하기 때문이다.\n",
    "\n",
    "<img src=\"img/107651920-60991600-6cc3-11eb-9bea-697a3236b03d.PNG\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버핏 방지\n",
    "오버핏은 모델이 트레이닝 데이터에 너무 최적화 되어 있어 테스트 데이터가 들어왔을때 유연하게 처리하지 못하는 문제를 말한다.\n",
    "\n",
    "딥러닝에서 오버핏을 방지하는 것은 엔지니어에게 주요 과제라고 할 수 있다.\\\n",
    "자 그럼 오버핏을 방지하는 여러가지 알고리즘들을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 감소\n",
    "예전부터 많이 사용하던 가중치 감소 알고리즘을 알아보자\n",
    "\n",
    "가중치 감소 알고리즘은 미분을 통해 가중치를 얼마나 조정해야 하는지 알아냈다면\\\n",
    "그 알아낸 가중치의 변경값을 적용하기 전에 L1, L2 ... Lp의 범위로 제한하여 너무 크게 적용되는 것을 막는다.\n",
    "\n",
    "참고로 L1, L2 ... Lp의 범위를 시각화 하면 다음과 같다.\n",
    "\n",
    "<img src=\"img/img.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "레이어가 너무 많아지면 가중치 감소 만으로 오버핏을 억제하기 어려울 수 있다.\n",
    "\n",
    "그래서 레이어의 랜덤하게 퍼셉트론을 몇개 죽이는 방식을 사용하는 것을 Dropout이라 한다.\\\n",
    "퍼셉트론을 죽이는 방법은 간단하게 순전파때는 출력을 0으로 하고 역전파때는 기울기를 0으로 하는 것이다.\n",
    "\n",
    "파이썬 코드로는 다음과 같이 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "  \"\"\"\n",
    "  http://arxiv.org/abs/1207.0580\n",
    "  \"\"\"\n",
    "  def __init__(self, dropout_ratio=0.5):\n",
    "    self.dropout_ratio = dropout_ratio\n",
    "    self.mask = None\n",
    "\n",
    "  def forward(self, x, train_flg=True):\n",
    "    if train_flg:\n",
    "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "      return x * self.mask\n",
    "    else:\n",
    "      return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "  def backward(self, dout):\n",
    "    return dout * self.mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
